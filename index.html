<!DOCTYPE html>
<html lang="en-gb">
    <head>
	<meta name="generator" content="Hugo 0.46" />
        <meta charset="utf-8" />

        
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

        <title>
             Not Much of a Blog
        </title>

        <link rel="stylesheet" href="https://tkngch.github.io/css/bootstrap.min.css" />
        <link rel="stylesheet" href="https://tkngch.github.io/css/main.css" />
        

    </head>

    <body class="colour04">
        <header class="global-header">
            <section class="header colour02Background">
                <a href="https://tkngch.github.io/">
                    <h1 class="colour01"> Not Much of a Blog </h1>
                    
                    <h5 class="colour03"> Bayesian Statistics, Behavioural Modeling, Machine Learning </h5>
                    
                </a>
            </section>
        </header>

        <main class="container">


<div class="article-list col-md-9">
    
    
    <article>

    <header>
        <h2 class="colour02">
            <a href="https://tkngch.github.io/post/2017-02-19_gradient-descent-by-gradient-descent/">Gradient Descent by Gradient Descent</a>
        </h2>

        <div class="pull-left small article-date colour03">
            Posted on
            <time datetime="2017-02-19T00:00:00Z">
                19 Feb 2017
            </time>
        </div>
    </header>

    <div class="clearfix"> </div>
    <div> <p>In this blog post, I review the 2016 NIPS paper &ldquo;learning to learn by gradient
descent by gradient descent&rdquo; (I abbreviate as LLGG) by Andrychowicz et al.</p>

<p></p> </div>

    
    <footer>
        <ul class="pager">
            <li class="next">
                <a href="https://tkngch.github.io/post/2017-02-19_gradient-descent-by-gradient-descent/">
                    Read more <span aria-hidden="true">&raquo;</span>
                </a>
            </li>
        </ul>
    </footer>
    

</article>

    
    
    <hr/>
    
    <article>

    <header>
        <h2 class="colour02">
            <a href="https://tkngch.github.io/post/2017-01-29_long-short-term-memory-network/">Long Short-Term Memory Network and Back-Propagation through Time</a>
        </h2>

        <div class="pull-left small article-date colour03">
            Posted on
            <time datetime="2017-01-29T00:00:00Z">
                29 Jan 2017
            </time>
        </div>
    </header>

    <div class="clearfix"> </div>
    <div> <p>Recurrent neural networks (RNNs) are neural networks to model sequential data.
RNNs are often used in speech recognition and natural language processing. In
this blog post, I discuss one of the most popular RNNs, a long short-term
memory (LSTM) network. Then I briefly address a training procedure for a LSTM.</p>

<p></p> </div>

    
    <footer>
        <ul class="pager">
            <li class="next">
                <a href="https://tkngch.github.io/post/2017-01-29_long-short-term-memory-network/">
                    Read more <span aria-hidden="true">&raquo;</span>
                </a>
            </li>
        </ul>
    </footer>
    

</article>

    
    
    <hr/>
    
    <article>

    <header>
        <h2 class="colour02">
            <a href="https://tkngch.github.io/post/2017-01-15_hidden-markov-model/">Hidden Markov Model</a>
        </h2>

        <div class="pull-left small article-date colour03">
            Posted on
            <time datetime="2017-01-15T00:00:00Z">
                15 Jan 2017
            </time>
        </div>
    </header>

    <div class="clearfix"> </div>
    <div> <p>Deep learning is quite popular in sequence modelling, but this blog post
discusses a more traditional model, a hidden Markov model.</p>

<p>(Updated on 18 March 2017)</p>

<p></p> </div>

    
    <footer>
        <ul class="pager">
            <li class="next">
                <a href="https://tkngch.github.io/post/2017-01-15_hidden-markov-model/">
                    Read more <span aria-hidden="true">&raquo;</span>
                </a>
            </li>
        </ul>
    </footer>
    

</article>

    
    
    <hr/>
    
    <article>

    <header>
        <h2 class="colour02">
            <a href="https://tkngch.github.io/post/2016-12-26_hierarchical-bayes/">Hierarchical Bayes</a>
        </h2>

        <div class="pull-left small article-date colour03">
            Posted on
            <time datetime="2016-12-26T00:00:00Z">
                26 Dec 2016
            </time>
        </div>
    </header>

    <div class="clearfix"> </div>
    <div> <p>Sometimes, data contain multiple entries from observation units. For example,
researchers may be interested in effectiveness of medical treatments, and
tested several treatments in several different cities. Then, it may make sense
to consider that the effectiveness may be overall consistent across cities but
slightly vary between the cities, as population characteristic may vary between
the cities. For instance, one city may have more younger people than another.
In these cases, hierarchical Bayes may help. In this blog post, I review
hierarchical Bayes.</p>

<p></p> </div>

    
    <footer>
        <ul class="pager">
            <li class="next">
                <a href="https://tkngch.github.io/post/2016-12-26_hierarchical-bayes/">
                    Read more <span aria-hidden="true">&raquo;</span>
                </a>
            </li>
        </ul>
    </footer>
    

</article>

    
    
    <hr/>
    
    <article>

    <header>
        <h2 class="colour02">
            <a href="https://tkngch.github.io/post/2016-12-17/">Dirichlet process mixture model as a cognitive model of category learning</a>
        </h2>

        <div class="pull-left small article-date colour03">
            Posted on
            <time datetime="2016-12-17T15:30:10Z">
                17 Dec 2016
            </time>
        </div>
    </header>

    <div class="clearfix"> </div>
    <div> <p>In cognitive science/psychology, Dirichlet process mixture model (DPMM) is
considered as a rational model of category learning (Anderson, 1991; Sanborn,
Griffiths &amp; Navarro, 2010). That is, the DPMM is used to approximate how human
learns to categorise objects.</p>

<p>In this post, I review the DPMM, assuming that all the features are discrete.
The implementation in C++ can be found
<a href="https://github.com/tkngch/dirichlet-process-mixture-model">here</a>.</p>

<p></p> </div>

    
    <footer>
        <ul class="pager">
            <li class="next">
                <a href="https://tkngch.github.io/post/2016-12-17/">
                    Read more <span aria-hidden="true">&raquo;</span>
                </a>
            </li>
        </ul>
    </footer>
    

</article>

    
</div>

<div class="sidebar col-md-3">

    
    <div class="sidebar-header colour02">
        <span>Author</span>
    </div>
    <div class="sidebar-content" id="author">
        <span>Takao Noguchi</span>
        <br/>
        
        <span class="small colour03">Research Scientist</span>
        
    </div>

    

    
    <div class="sidebar-header colour02">
        <span>Tags</span>
    </div>
    <div class="sidebar-content article-tag">
        <ul class="list-unstyled">
            
            <li class="sidebar-tag-list">
                <a href="/tags/bayesian">
                    <span></span>
                    #bayesian (2)
                </a>
            </li>
            
            <li class="sidebar-tag-list">
                <a href="/tags/cognitive-science">
                    <span></span>
                    #cognitive-science (1)
                </a>
            </li>
            
            <li class="sidebar-tag-list">
                <a href="/tags/deep-learning">
                    <span></span>
                    #deep-learning (2)
                </a>
            </li>
            
            <li class="sidebar-tag-list">
                <a href="/tags/optimization">
                    <span></span>
                    #optimization (3)
                </a>
            </li>
            
            <li class="sidebar-tag-list">
                <a href="/tags/time-series">
                    <span></span>
                    #time-series (3)
                </a>
            </li>
            
        </ul>
    </div>
    

</div>

<nav class="pagination" role="navigation">

    

    

    <span class="page-number">Page 1 of 1</span>

    

</nav>

        </main>

        <footer class="container global-footer">
            <div class="pull-left small colour03">
                &copy; 2016 Takao Noguchi. Creative Commons Attribution 4.0 International License.
            </div>
        </footer>

        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [['$','$'], ['\\(','\\)']],
                    displayMath: [['$$','$$'], ['\[','\]']]
                }
            });
        </script>

    </body>
</html>

