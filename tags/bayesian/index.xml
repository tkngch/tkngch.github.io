<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on Not Much of a Blog</title>
    <link>https://tkngch.github.io/tags/bayesian/index.xml</link>
    <description>Recent content in Bayesian on Not Much of a Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2016 Takao Noguchi. Creative Commons Attribution 4.0 International License.</copyright>
    <atom:link href="https://tkngch.github.io/tags/bayesian/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Hierarchical Bayes</title>
      <link>https://tkngch.github.io/post/2016-12-26_Hierarchical-Bayes/</link>
      <pubDate>Mon, 26 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2016-12-26_Hierarchical-Bayes/</guid>
      <description>&lt;p&gt;Sometimes, data contain multiple entries from observation units. For example,
researchers may be interested in effectiveness of medical treatments, and
tested several treatments in several different cities. Then, it may make sense
to consider that the effectiveness may be overall consistent across cities but
slightly vary between the cities, as population characteristic may vary between
the cities. For instance, one city may have more younger people than another.
In these cases, hierarchical Bayes may help. In this blog post, I review
hierarchical Bayes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;For the illustration purposes, let&amp;rsquo;s say we want to fit a linear regression:
[
    y \sim \mathcal{N}(X \beta, \epsilon).
]
In the above example of medical treatments, each row in $X$ corresponds to one
observation. Here, $X$ indexes the type of treatment received, and $y$ is a
measure of effectiveness.&lt;/p&gt;

&lt;p&gt;For convenience, I make the condition on $X$ implicit in this blog post.&lt;/p&gt;

&lt;h2 id=&#34;complete-pooling&#34;&gt;Complete pooling&lt;/h2&gt;

&lt;p&gt;Before diving into hierarchical Bayes, it is useful to discuss simpler
approaches. One approach is to ignore the possibility that the effectiveness of
treatments may vary between the cities, and assume that the effectiveness is
identical across the cities. This approach is called &lt;em&gt;complete pooling&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;First, we define the prior distribution for the parameter:
[
    \beta \sim \mathcal{N}(\mu, \sigma).
]
Here, $\mu$ and $\sigma$ are hyperparameters &amp;mdash; something you have to specify.
How to choose hyper-parameters is a topic for another blog post.&lt;/p&gt;

&lt;p&gt;After specifying the prior distribution, we can optimise $\beta$ according to
its posterior distribution:
[
    p(\beta \vert y) = \frac{p(\beta) \; p(y \vert \beta)}{p(y).}
]
The denominator, $p(y)$, is usually difficult to compute, but it does not
depend on $\beta$, so often we just consider the unnormalised posterior:
[
    p(\beta \vert y) \propto p(\beta) \; p(y \vert \beta).
]&lt;/p&gt;

&lt;h2 id=&#34;no-pooling&#34;&gt;No pooling&lt;/h2&gt;

&lt;p&gt;Another simpler approach is to acknowledge the possibility that the
effectiveness of treatments varies between the cities and allow the
effectiveness to vary as much as possible between the cities. Thus, this
approach fits the model independently for each city. This approach is called
&lt;em&gt;no pooling&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Here the model for the $i$th city is
[
    y^{(i)} \sim \mathcal{N}(X^{(i)} \beta^{(i)}, \epsilon^{(i)}).
]&lt;/p&gt;

&lt;p&gt;We specify the prior as before:
[
    \beta^{(i)} \sim \mathcal{N}(\mu, \sigma).
]
And then, optimise $\beta^{(i)}$ according to the posterior $p(\beta^{(i)}
\vert y)$.&lt;/p&gt;

&lt;h2 id=&#34;partial-pooling-hierarchical-bayes&#34;&gt;Partial pooling (hierarchical Bayes)&lt;/h2&gt;

&lt;p&gt;In some cases, it makes sense to allow the effectiveness to vary between the
cities but also to assume that the effectiveness is somewhat consistent across
the cities. This is where the third approach, &lt;em&gt;partial pooling&lt;/em&gt; comes in. The
partial pooling is often called hierarchical Bayes.&lt;/p&gt;

&lt;p&gt;As before, let&amp;rsquo;s say the model for the $i$th city is
[
    y^{(i)} \sim X^{(i)} \beta^{(i)} + \epsilon^{(i)},
]
and the prior is
[
    \beta^{(i)} \sim \mathcal{N}(\mu, \sigma).
]&lt;/p&gt;

&lt;p&gt;The trick is to treat $\mu$ and $\sigma$ here as parameters, as opposed to
hyperparameters. That is, we let the data to inform us of $\mu$ and $\sigma$.
For this, we need to specify the prior for $\mu$ and $\sigma$. My usual choice is
[
    \mu \sim Uniform(-inf, inf)
]
and
[
    \sigma \sim Uniform(0, inf).
]
These are non-informative distributions. These are also improper, as their
integral is not one. While the prior is improper, the posterior integrates to
one, so it doesn&amp;rsquo;t cause much practical problems.&lt;/p&gt;

&lt;p&gt;Then, we can work out the joint posterior distribution.
[
\begin{align}
    p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y)
        &amp;amp;= \frac{p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma) \; p(y \vert \beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma)}{p(y)} \\&lt;br /&gt;
        &amp;amp;= \frac{p(\mu, \sigma) \; p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)} \vert \mu, \sigma) \; p(y \vert \beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)})}{p(y).}
\end{align}
]
As before, we often only need to consider the unnormalised posterior:
[
    p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y) \propto
        p(\mu, \sigma) \; p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)} \vert \mu, \sigma) \; p(y \vert \beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}).
]&lt;/p&gt;

&lt;p&gt;The overall effectiveness of treatments can be inferred by integrating out
$\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}$ from the above equation:
[
    p(\mu, \sigma \vert y) =
        \int
            p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y)
        d_{\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}}.
]
Similarly, the effectiveness for each city can be inferred by integrating out
$\mu$ and $\sigma$:
[
    p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)} \vert y) =
        \int \int
            p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y)
        d_{\mu} d_{\sigma}.
]&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dirichlet process mixture model as a cognitive model of category learning</title>
      <link>https://tkngch.github.io/post/2016-12-17/</link>
      <pubDate>Sat, 17 Dec 2016 15:30:10 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2016-12-17/</guid>
      <description>&lt;p&gt;In cognitive science/psychology, Dirichlet process mixture model (DPMM) is
considered as a rational model of category learning (Anderson, 1991; Sanborn,
Griffiths &amp;amp; Navarro, 2010). That is, the DPMM is used to approximate how human
learns to categorise objects.&lt;/p&gt;

&lt;p&gt;In this post, I review the DPMM, assuming that all the features are discrete.
The implementation in C++ can be found
&lt;a href=&#34;https://github.com/tkngch/dirichlet-process-mixture-model&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Suppose a learner has observed $n - 1$ objects $\{x_{1}, x_{2}, \dots, x_{n -
1}\}$ with corresponding category labels $\{y_{1}, y_{2}, \dots, y_{n - 1}\}$.
In the DPMM, each of these objects fits into a cluster. The cluster label for
the $i$th object is denoted as $z_{i}$.&lt;/p&gt;

&lt;h2 id=&#34;drawing-an-inference&#34;&gt;Drawing an inference&lt;/h2&gt;

&lt;p&gt;Then, the probability that the $n$th object fits into category $w$ is expressed
as follows:&lt;/p&gt;

&lt;p&gt;[
\begin{align}
    p(y_{n} = w \; \vert \; x_{n})
    &amp;amp;= \sum_{k \in \mathbb{Z}} p(z_{n} = k \; \vert \; x_{n}) \; p(y_{n} = w \; \vert \; z_{n} = k) \\&lt;br /&gt;
    &amp;amp;= \sum_{k \in \mathbb{Z}} \; \frac{p(z_{n} = k) \; p(x_{n} \;\vert\; z_{n} = k)}{p(x_{n})} \; p(y_{n} = w \; \vert \; z_{n} =
    k) \\&lt;br /&gt;
    &amp;amp;= \sum_{k \in \mathbb{Z}} \; \frac{p(z_{n} = k) \; p(x_{n} \;\vert\; z_{n} = k)}{\sum_{s \in
    \mathbb{Z}} \; p(z_{n} = s) \; p(x_{n} \;\vert\; z_{n} = s)} \; p(y_{n} = w \; \vert \; z_{n} = k).
\label{eqn:inference} \tag{1}
\end{align}
]&lt;/p&gt;

&lt;p&gt;Here, $\mathbb{Z}$ is a set of all the possible clusters to which the $n$th
object can be assigned.  The three terms in Equation $\ref{eqn:inference}$ are
described below in turn.&lt;/p&gt;

&lt;p&gt;First, the probability that the $n$th object fits into a cluster $k$ is given
by:
[
    p(z_{n} = k) =
        \begin{cases}
            \frac{c\,m_{k}}{(1 - c) + c\,(n - 1)} &amp;amp; \text{if $m_{k} &amp;gt; 0$}\\&lt;br /&gt;
            \\&lt;br /&gt;
            \frac{(1 - c)}{(1 - c) + c\,(n - 1)} &amp;amp; \text{if $m_{k} = 0$}
        \end{cases}
\label{eqn:prior}\tag{2}
]
where $c$ is a parameter called the coupling probability and $m_{k}$ is the
number of objects assigned into cluster $k$.&lt;/p&gt;

&lt;p&gt;In typical experiments in cognitive science/psychology, each feature is
independent.  Therefore,
[
    p(x_{n} \; \vert \; z_{n} = k) = \prod_{d \in D} p(x_{n,d} \; \vert \; z_{n} = k),
\label{eqn:feature1}\tag{3}
]
where $D$ is a set of features that describe $x$.  The above term is computed
with
[
    p(x_{n,d} = v \; \vert \; z = k) = \frac{B_{v,d} + \beta_{c}}{m_{k} + J_{d} \, \beta_{c}},
\label{eqn:feature2}\tag{4}
]
where $B_{v,d}$ is the number of objects in cluster $k$ with value of $v$ on
feature $d$, and $J_{d}$ is the number of values which an object can take on
feature $d$.&lt;/p&gt;

&lt;p&gt;Similarly, the probability that the $n$th object has category label $w$, given
a cluster, is given by:
[
    p(y_{n} = w \; \vert \; z = k) = \frac{B_{w} + \beta_{l}}{B_{.} + J \, \beta_{l}},
\label{eqn:label}\tag{5}
]
where $B_{w}$ is the number of observed objects with category label $w$ in
cluster $k$, $B_{.}$ is the number of objects in cluster $k$, and $J$ is the
number of possible category labels.&lt;/p&gt;

&lt;h2 id=&#34;learning&#34;&gt;Learning&lt;/h2&gt;

&lt;p&gt;The learning in the DPMM is equivalent to assigning an object into a cluster.
The cluster assignment of the $n$th object, for example, is in accordance with
the probability of each cluster. Specifically, an object is assigned to a
cluster with the probability:
[
    p(z_{n} = k \;\vert\; x_{n},\, y_{n}) \propto
    p(z_{n} = k)
    \;
    p(x_{n} \;\vert\; z_{n} = k)
    \;
    p(y_{n} \;\vert\; z_{n} = k)
\label{eqn:learning}\tag{6}
]
This is computed with Equations $\ref{eqn:prior}$, $\ref{eqn:feature1}$ and
$\ref{eqn:label}$.&lt;/p&gt;

&lt;p&gt;This learning is mathematically intractable and approximated with local MAP
(Anderson, 1991) and particle filter (Sanborn et al., 2010).&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;Anderson, J. R. (1991). The adaptive nature of human categorization.
&lt;em&gt;Psychological Review, 98,&lt;/em&gt; 409-29.&lt;/p&gt;

&lt;p&gt;Sanborn, A. N., Griffiths, T. L., &amp;amp; Navarro, D. J. (2010). Rational
approximations to rational models: alternative algorithms for category
learning. &lt;em&gt;Psychological Review, 117,&lt;/em&gt; 1144-1167.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>