<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Not Much of a Blog</title>
    <link>https://tkngch.github.io/tags/deep-learning/index.xml</link>
    <description>Recent content in Deep Learning on Not Much of a Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2016 Takao Noguchi. Creative Commons Attribution 4.0 International License.</copyright>
    <atom:link href="https://tkngch.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Gradient Descent by Gradient Descent</title>
      <link>https://tkngch.github.io/post/2017-02-19_Gradient-Descent-by-Gradient-Descent/</link>
      <pubDate>Sun, 19 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-02-19_Gradient-Descent-by-Gradient-Descent/</guid>
      <description>&lt;p&gt;In this blog post, I review the 2016 NIPS paper &amp;ldquo;learning to learn by gradient
descent by gradient descent&amp;rdquo; (I abbreviate as LLGG) by Andrychowicz et al.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Gradient descent is an iterative optimization algorithm. Consider the problem
of minimizing an objective function $f(\theta)$. Gradient descent solves this
problem through a sequence of updates:
[
    \theta_{t+1} = \theta_{t} + \alpha_{t} \; \nabla_{\theta_{t}} f
]
where $\nabla$ denotes gradient, $\nabla_{\theta_{t}} f = \frac{\partial
f}{\partial \theta_{t}}$.&lt;/p&gt;

&lt;p&gt;In LLGG, the update $\alpha_{t} \; \nabla_{\theta_{t}} f$ is replaced with
an output from the optimizer function $g$:
[
    \theta_{t+1} = \theta_{t} + g_{t}.
]&lt;/p&gt;

&lt;h2 id=&#34;optimizer-function&#34;&gt;Optimizer function&lt;/h2&gt;

&lt;p&gt;For the optimizer function, a two-layer stack of LSTM networks is used:
[
\begin{align}
    h^{(t)} &amp;amp;= LSTM_{1}(\nabla_{\theta_{t}} f)\\&lt;br /&gt;
    g_{t} &amp;amp;= LSTM_{2}(h^{(t)}).
\end{align}
]
The number of dimensions for the hidden node is set at 20 in the LLGG paper.&lt;/p&gt;

&lt;p&gt;This optimizer function is trained at the same time, using the truncated
back-propagation through time. When $\theta$ is updated $t$ times, the loss
function is
[
    L = \sum_{s=t - T}^{t} f\left( \theta_{s} \right),
]
where in the LLGG paper, the truncation length $T$ is 20 or 32, depending on the
training data.&lt;/p&gt;

&lt;p&gt;Then, the set of the parameters for the LSTM stack, $\phi$, is updated as follows:
[
    \phi_{s+1} = \phi_{s} + \alpha_{s} \, \frac{\partial L}{\partial \phi_{s}},
]
where $\alpha$ is given by the adaptive moment estimation algorithm, and
[
\begin{align}
    \frac{\partial L}{\partial \phi_{s}}
    &amp;amp;= \sum_{t} \left(
    \frac{\partial L}{\partial g_{t}}
        \frac{\partial g_{t}}{\partial \phi_{s}}
    + \frac{\partial L}{\partial \nabla_{\theta_{t-1}}f}
        \frac{\partial \nabla_{\theta_{t-1}}f}{\partial \phi_{s}}
    \right).
\end{align}
]
For computational feasibility, it is assumed that
[
    \frac{\partial \nabla_{\theta_{t}}}{\partial \phi} = 0
]
so that the LSTM stack can be trained with the standard, back-propagation
through time.&lt;/p&gt;

&lt;p&gt;Summing up, the training procedure is&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialise $\theta$ and $\phi$&lt;/li&gt;
&lt;li&gt;For $t = 1, 2, \dots$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $L \gets 0$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ For $s = 1, 2, \dots, T$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\hspace{1cm}$ $L \gets L + f(\theta)$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\hspace{1cm}$ $g_{s} = m(\nabla_{\theta}f, \, \phi)$, where $m$ is the optimizer function (i.e., the stack of LSTM networks)&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\hspace{1cm}$ $\theta \gets \theta + g_{s}$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\phi \gets \phi + \alpha \, \frac{\partial L}{\partial \phi}$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ Update $\alpha$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For computational feasibility again, the optimizer function is
&lt;em&gt;coordinatewise&lt;/em&gt;: each optimizer function outputs a scaler to update each
parameter individually. Here, the same optimizer function is used for each
parameter.&lt;/p&gt;

&lt;p&gt;Though, the gradient for each parameter can vary greatly, which can be
problematic for training the optimizer function. As a remedy, the LLGG paper
pre-processes the input to the optimizer function:
[
    \nabla \rightarrow
        \begin{cases}
            \left( \frac{\ln \vert \nabla \vert}{p},\, sign(\nabla) \right) &amp;amp;
            \textrm{if } \vert \nabla \vert \geq \exp(-p)
            \\&lt;br /&gt;
            \left(-1, \exp(p \, \nabla) \right) &amp;amp;
            \textrm{otherwise}
        \end{cases}
]
where $p&amp;gt;0$ controls how small gradients are disregarded. The LLGG paper uses
$p=10$.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., et al. (2016). Learning
to learn by gradient descent by gradient descent. &lt;a href=&#34;https://arxiv.org/abs/1606.04474&#34;&gt;arXiv:1606.04474
(cs.NE)&lt;/a&gt;. (Code is available at &lt;a href=&#34;https://github.com/deepmind/learning-to-learn&#34;&gt;their GitHub
repo&lt;/a&gt;.)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory Network and Back-Propagation through Time</title>
      <link>https://tkngch.github.io/post/2017-01-29_Long-Short-Term-Memory-Network/</link>
      <pubDate>Sun, 29 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-01-29_Long-Short-Term-Memory-Network/</guid>
      <description>&lt;p&gt;Recurrent neural networks (RNNs) are neural networks to model sequential data.
RNNs are often used in speech recognition and natural language processing. In
this blog post, I discuss one of the most popular RNNs, a long short-term
memory (LSTM) network. Then I briefly address a training procedure for a LSTM.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;long-short-term-memory-network&#34;&gt;Long short-term memory network&lt;/h2&gt;

&lt;p&gt;While RNNs in theory can use contextual information to map input to output
sequences, RNNs in practice can use only a limited range of context. The basic
problem is that error signals propagated through time tend to either vanish or
explode. This difficulty arises from the exponentially smaller weights given to
long-term interactions compared to short-term ones (the vanishing gradient
problem). As a remedy to the problem, an LSTM network was proposed in 1997.&lt;/p&gt;

&lt;p&gt;An LSTM is a special kind of RNNs, and its core contribution is an introduction
of a self-connected unit (&lt;em&gt;a memory cell unit&lt;/em&gt;). The weight of this cell is
gated, as opposed to fixed, which enables the error signals to flow for long
duration. An LSTM contains three gates: a forget gate and an input gate to
control what information is stored at the memory cell unit, and an output gate
to control what information is accessed at the memory cell unit.&lt;/p&gt;

&lt;p&gt;The forget gate unit for time step $t$ takes $\bar{f}^{(t)}$ as an input
[
    \bar{f}^{(t)} = b_{f} + U_{f} \; x^{(t)} + W_{f} \; h^{(t-1)},
]
and gets $f^{(t)}$ as an activation
[
    f^{(t)} = \sigma_{f} \left( \bar{f}^{(t)} \right).
]
Here, $\sigma_{f}$ is a sigmoid, usually the logistic function;
$x^{(t)}$ is the input vector and $h^{(t)}$ is the hidden layer vector
for time $t$; and $b_{f}$, $U_{f}$, and $W_{f}$ are biases, input weights
and recurrent weights for the forget gate.&lt;/p&gt;

&lt;p&gt;The activation of the forget gate, $f^{(t)}$, can take a value between 0 and 1,
where 1 signifies &amp;ldquo;retain all of the memory cell state&amp;rdquo; and 0 signifies
&amp;ldquo;completely forget the memory cell state.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The input gate unit is computed similarly to the forget gate, but with its own
parameters. Its input is
[
    \bar{g}^{(t)} = b_{g} + U_{g} \; x^{(t)} + W_{g} \; h^{(t-1)},
]
and the output is
[
    g^{(t)} = \sigma_{g} \left( \bar{g}^{(t)} \right).
]
Here again, $\sigma_{g}$ is a sigmoid, usually the logistic function.
The activation of this input gate, $g^{(t)}$, can take a value between 0 and 1,
and determines which memory cell units are updated and how much they are
updated.&lt;/p&gt;

&lt;p&gt;The memory cell unit also gets an input from the block input unit. An input to
the block input is
[
    \bar{z}^{(t)} = b_{z} + U_{z} \; x^{(t)} + W_{z} \; h^{(t-1)},
]
and its activation is
[
    z^{(t)} = \sigma_{z} \left( \bar{z}^{(t)} \right),
]
where  $\sigma_{z}$ is usually the hyperbolic tangent function.&lt;/p&gt;

&lt;p&gt;Then, the memory cell state is updated as follows:
[
    s^{(t)} = f^{(t)} \odot s^{(t-1)} + g^{(t)} \odot z^{(t)},
]
where $\odot$ indicates element-wise (Hadamard) product.&lt;/p&gt;

&lt;p&gt;Remaining is the output gate. Its input is
[
    \bar{q}^{(t)} = b_{q} + U_{q} \; x^{(t)} + W_{q} \; h^{(t-1)},
]
and its activation is
[
    q^{(t)} = \sigma_{q} \left( \bar{q}^{(t)} \right).
]
Here again, $\sigma_{q}$ is a sigmoid, usually the logistic function.
This output gate determines which memory cell states contributes to the hidden
state and the output.&lt;/p&gt;

&lt;p&gt;Then, the hidden state is updated:
[
    h^{(t)} = \sigma_{h} \left( s^{(t)} \right) \odot q^{(t)},
]
where $\sigma_{h}$ is usually the hyperbolic tangent function. The output
from a LSTM is this hidden state vector, $h^{(t)}$.&lt;/p&gt;

&lt;p&gt;Therefore, a LSTM accepts a sequence of inputs $x^{(t)}$ and outputs a sequence
$h^{(t)}$, which I symbolically write as
[
    h = LSTM(x).
]
In practice, a LSTM is often stacked. A 4-layer LSTM network, for example, is
[
\begin{align}
    h^{(t, 1)} &amp;amp;= LSTM_{1}(x^{(t)})\\&lt;br /&gt;
    h^{(t, 2)} &amp;amp;= LSTM_{2}(h^{(t, 1)})\\&lt;br /&gt;
    h^{(t, 3)} &amp;amp;= LSTM_{3}(h^{(t, 2)})\\&lt;br /&gt;
    h^{(t, 4)} &amp;amp;= LSTM_{4}(h^{(t, 3)}).
\end{align}
]
where $h^{(t, 4)}$ is used to compute the loss function at time $t$.&lt;/p&gt;

&lt;h2 id=&#34;back-propagation-through-time&#34;&gt;Back-propagation through time&lt;/h2&gt;

&lt;p&gt;The original LSTM training algorithm (Hochreiter and Schmidhuber, 1997) used an
approximate error gradient calculated with a combination of real time recurrent
learning and back-propagation through time (BPTT). However, it is possible to
calculate the exact LSTM gradient with BPTT.&lt;/p&gt;

&lt;p&gt;For BPTT, we recursively compute gradient. Let me introduce new notations: $L$
denotes the loss function and $\tau$ is a sequence length.  Then, we derive the
gradient with respect to $h$, $s$, $\bar{q}$, $\bar{z}$, $\bar{g}$, and $\bar{f}$.
For convenience, I introduce a few notations:
[
\begin{align}
    \epsilon_{h}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial h^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{s}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial s^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{q}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{q}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{q}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{q}^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{z}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{z}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{z}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{z}^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{g}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{g}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{g}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{g}^{(t)}}
    \quad{\textrm{and}}
    \\&lt;br /&gt;
    \epsilon_{\bar{f}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{f}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{f}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{f}^{(t)}.}
\end{align}
]&lt;/p&gt;

&lt;p&gt;We first compute
[
    \epsilon_{h}^{(\tau)} = \frac{\partial L^{(\tau)}}{\partial h^{(\tau)}}
]
and
[
\begin{align}
    \epsilon_{s}^{(\tau)}
        &amp;amp;= \frac{\partial L^{(\tau)}}{\partial s^{(\tau)}}\\&lt;br /&gt;
        &amp;amp;=
            \frac{\partial L^{(\tau)}}{\partial h^{(\tau)}}
            \frac{\partial h^{(\tau)}}{\partial \sigma_{s} \left(s^{(\tau)} \right)}
            \frac{\partial \sigma_{s} \left(s^{(\tau)} \right)}{\partial s^{(\tau)}}
        \\&lt;br /&gt;
        &amp;amp;=
            \epsilon_{h}^{(\tau)}
            \odot
            q^{(\tau)}
            \odot
            \sigma_{s}&amp;rsquo;\left(s^{(\tau)} \right).
\end{align}
]&lt;/p&gt;

&lt;p&gt;Then, we go backwards through time by computing for $t = \tau - 1 , \tau - 2, \dots, 1$:
[
\begin{align}
    \epsilon_{s}^{(t)}
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial s^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \epsilon_{s}^{(t+1)} \odot f^{(t+1)}
        \\&lt;br /&gt;
        &amp;amp;= \epsilon_{h}^{(t)} \odot q^{(t)} \odot \sigma_{h}&amp;rsquo;\left(s^{(t)} \right) +
            \epsilon_{s}^{(t+1)} \odot f^{(t+1)},
\end{align}
]
and
[
\begin{align}
    \epsilon_{h}^{(t)}
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            \epsilon_{\bar{q}}^{(t+1)} \frac{\partial \bar{q}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{z}}^{(t+1)} \frac{\partial \bar{z}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{g}}^{(t+1)} \frac{\partial \bar{g}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{f}}^{(t+1)} \frac{\partial \bar{f}^{(t+1)}}{\partial h^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            W_{q}^{T} \epsilon_{\bar{q}}^{(t+1)} +
            W_{z}^{T} \epsilon_{\bar{z}}^{(t+1)} +
            W_{g}^{T} \epsilon_{\bar{g}}^{(t+1)} +
            W_{f}^{T} \epsilon_{\bar{f}}^{(t+1)},
\end{align}
]
where superscript $T$ indicates transpose.&lt;/p&gt;

&lt;p&gt;The gradient with respect to other functions is given by
[
\begin{align}
    \epsilon_{\bar{q}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t+1)}}
            \frac{\partial h^{(t+1)}}{\partial q^{(t+1)}}
            \frac{\partial q^{(t+1)}}{\partial \bar{q}^{(t+1)}}
        =
        \epsilon_{h}^{(t+1)} \odot
        \sigma_{h} \left( s^{(t+1)} \right) \odot
        \sigma_{q}&amp;rsquo; \left( \bar{q}^{(t+1)} \right),
    \\&lt;br /&gt;
    \epsilon_{\bar{z}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial z^{(t+1)}}
            \frac{\partial z^{(t+1)}}{\partial \bar{z}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        g^{(t)} \odot
        \sigma_{z}&amp;rsquo; \left( \bar{z}^{(t+1)} \right),
    \\&lt;br /&gt;
    \epsilon_{\bar{g}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial g^{(t+1)}}
            \frac{\partial g^{(t+1)}}{\partial \bar{g}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        z^{(t)} \odot
        \sigma_{g}&amp;rsquo; \left( \bar{g}^{(t+1)} \right), \textrm{ and }
    \\&lt;br /&gt;
    \epsilon_{\bar{f}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial f^{(t+1)}}
            \frac{\partial f^{(t+1)}}{\partial \bar{f}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        s^{(t)} \odot
        \sigma_{f}&amp;rsquo; \left( \bar{f}^{(t+1)} \right).
\end{align}
]&lt;/p&gt;

&lt;p&gt;The gradient for each parameter is computed with the above terms. For example,
[
    \frac{\partial L}{\partial W_{f}}
        = \sum_{t=1}^{\tau} \frac{\partial L}{\partial \bar{f}^{(t)}} \frac{\partial \bar{f}^{(t)}}{\partial W_{f}}
        = \sum_{t=1}^{\tau} \epsilon_{\bar{f}}^{(t)} \otimes x^{(t)},
]
where $\otimes$ indicates outer product.&lt;/p&gt;

&lt;p&gt;For a multi-layer LSTM, we need to compute the gradient with respect to the
input $x$.
[
\begin{align}
    \frac{\partial L}{\partial x^{(t)}}
    &amp;amp;=
        \frac{\partial L}{\partial \bar{f}^{(t)}}
        \frac{\partial \bar{f}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{g}^{(t)}}
        \frac{\partial \bar{g}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{z}^{(i)}}
        \frac{\partial \bar{z}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{q}^{(t)}}
        \frac{\partial \bar{q}^{(t)}}{\partial x^{(t)}}
    \\&lt;br /&gt;
    &amp;amp;=
        W_{f}^{T} \, \epsilon_{\bar{f}}^{(t)}
        +
        W_{g}^{T} \, \epsilon_{\bar{g}}^{(t)}
        +
        W_{z}^{T} \, \epsilon_{\bar{z}}^{(t)}
        +
        W_{q}^{T} \, \epsilon_{\bar{q}}^{(t)}
\end{align}
]&lt;/p&gt;

&lt;h2 id=&#34;references-and-bibliography&#34;&gt;References and Bibliography&lt;/h2&gt;

&lt;p&gt;Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. &lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;link to
the book website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Graves, A. (2012). Supervised sequence labelling with recurrent neural
networks. Springer. &lt;a href=&#34;http://www.cs.toronto.edu/~graves/preprint.pdf&#34;&gt;link to preprint
pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., and Schmidhuber,
J. (2015). LSTM: A Search Space Odyssey. &lt;a href=&#34;https://arxiv.org/abs/1503.04069&#34;&gt;arXiv:1503.04069
(cs.NE)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. &lt;em&gt;Neural
computation, 9,&lt;/em&gt; 1735-1780.&lt;/p&gt;

&lt;p&gt;Jimenez, N. D.  (2014). Simple LSTM. &lt;a href=&#34;http://nicodjimenez.github.io/2014/08/08/lstm.html&#34;&gt;link to the blog
post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Olah, C. (2015). Understanding LSTM networks. &lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;link to the blog
post&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>