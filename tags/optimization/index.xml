<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Optimization on Not Much of a Blog</title>
    <link>https://tkngch.github.io/tags/optimization/index.xml</link>
    <description>Recent content in Optimization on Not Much of a Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2016 Takao Noguchi. Creative Commons Attribution 4.0 International License.</copyright>
    <atom:link href="https://tkngch.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Gradient Descent by Gradient Descent</title>
      <link>https://tkngch.github.io/post/2017-02-19_Gradient-Descent-by-Gradient-Descent/</link>
      <pubDate>Sun, 19 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-02-19_Gradient-Descent-by-Gradient-Descent/</guid>
      <description>&lt;p&gt;In this blog post, I review the 2016 NIPS paper &amp;ldquo;learning to learn by gradient
descent by gradient descent&amp;rdquo; (I abbreviate as LLGG) by Andrychowicz et al.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Gradient descent is an iterative optimization algorithm. Consider the problem
of minimizing an objective function $f(\theta)$. Gradient descent solves this
problem through a sequence of updates:
[
    \theta_{t+1} = \theta_{t} + \alpha_{t} \; \nabla_{\theta_{t}} f
]
where $\nabla$ denotes gradient, $\nabla_{\theta_{t}} f = \frac{\partial
f}{\partial \theta_{t}}$.&lt;/p&gt;

&lt;p&gt;In LLGG, the update $\alpha_{t} \; \nabla_{\theta_{t}} f$ is replaced with
an output from the optimizer function $g$:
[
    \theta_{t+1} = \theta_{t} + g_{t}.
]&lt;/p&gt;

&lt;h2 id=&#34;optimizer-function&#34;&gt;Optimizer function&lt;/h2&gt;

&lt;p&gt;For the optimizer function, a two-layer stack of LSTM networks is used:
[
\begin{align}
    h^{(t)} &amp;amp;= LSTM_{1}(\nabla_{\theta_{t}} f)\\&lt;br /&gt;
    g_{t} &amp;amp;= LSTM_{2}(h^{(t)}).
\end{align}
]
The number of dimensions for the hidden node is set at 20 in the LLGG paper.&lt;/p&gt;

&lt;p&gt;This optimizer function is trained at the same time, using the truncated
back-propagation through time. When $\theta$ is updated $t$ times, the loss
function is
[
    L = \sum_{s=t - T}^{t} f\left( \theta_{s} \right),
]
where in the LLGG paper, the truncation length $T$ is 20 or 32, depending on the
training data.&lt;/p&gt;

&lt;p&gt;Then, the set of the parameters for the LSTM stack, $\phi$, is updated as follows:
[
    \phi_{s+1} = \phi_{s} + \alpha_{s} \, \frac{\partial L}{\partial \phi_{s}},
]
where $\alpha$ is given by the adaptive moment estimation algorithm, and
[
\begin{align}
    \frac{\partial L}{\partial \phi_{s}}
    &amp;amp;= \sum_{t} \left(
    \frac{\partial L}{\partial g_{t}}
        \frac{\partial g_{t}}{\partial \phi_{s}}
    + \frac{\partial L}{\partial \nabla_{\theta_{t-1}}f}
        \frac{\partial \nabla_{\theta_{t-1}}f}{\partial \phi_{s}}
    \right).
\end{align}
]
For computational feasibility, it is assumed that
[
    \frac{\partial \nabla_{\theta_{t}}}{\partial \phi} = 0
]
so that the LSTM stack can be trained with the standard, back-propagation
through time.&lt;/p&gt;

&lt;p&gt;Summing up, the training procedure is&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialise $\theta$ and $\phi$&lt;/li&gt;
&lt;li&gt;For $t = 1, 2, \dots$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $L \gets 0$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ For $s = 1, 2, \dots, T$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\hspace{1cm}$ $L \gets L + f(\theta)$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\hspace{1cm}$ $g_{s} = m(\nabla_{\theta}f, \, \phi)$, where $m$ is the optimizer function (i.e., the stack of LSTM networks)&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\hspace{1cm}$ $\theta \gets \theta + g_{s}$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ $\phi \gets \phi + \alpha \, \frac{\partial L}{\partial \phi}$&lt;/li&gt;
&lt;li&gt;$\hspace{1cm}$ Update $\alpha$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For computational feasibility again, the optimizer function is
&lt;em&gt;coordinatewise&lt;/em&gt;: each optimizer function outputs a scaler to update each
parameter individually. Here, the same optimizer function is used for each
parameter.&lt;/p&gt;

&lt;p&gt;Though, the gradient for each parameter can vary greatly, which can be
problematic for training the optimizer function. As a remedy, the LLGG paper
pre-processes the input to the optimizer function:
[
    \nabla \rightarrow
        \begin{cases}
            \left( \frac{\ln \vert \nabla \vert}{p},\, sign(\nabla) \right) &amp;amp;
            \textrm{if } \vert \nabla \vert \geq \exp(-p)
            \\&lt;br /&gt;
            \left(-1, \exp(p \, \nabla) \right) &amp;amp;
            \textrm{otherwise}
        \end{cases}
]
where $p&amp;gt;0$ controls how small gradients are disregarded. The LLGG paper uses
$p=10$.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., et al. (2016). Learning
to learn by gradient descent by gradient descent. &lt;a href=&#34;https://arxiv.org/abs/1606.04474&#34;&gt;arXiv:1606.04474
(cs.NE)&lt;/a&gt;. (Code is available at &lt;a href=&#34;https://github.com/deepmind/learning-to-learn&#34;&gt;their GitHub
repo&lt;/a&gt;.)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory Network and Back-Propagation through Time</title>
      <link>https://tkngch.github.io/post/2017-01-29_Long-Short-Term-Memory-Network/</link>
      <pubDate>Sun, 29 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-01-29_Long-Short-Term-Memory-Network/</guid>
      <description>&lt;p&gt;Recurrent neural networks (RNNs) are neural networks to model sequential data.
RNNs are often used in speech recognition and natural language processing. In
this blog post, I discuss one of the most popular RNNs, a long short-term
memory (LSTM) network. Then I briefly address a training procedure for a LSTM.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;long-short-term-memory-network&#34;&gt;Long short-term memory network&lt;/h2&gt;

&lt;p&gt;While RNNs in theory can use contextual information to map input to output
sequences, RNNs in practice can use only a limited range of context. The basic
problem is that error signals propagated through time tend to either vanish or
explode. This difficulty arises from the exponentially smaller weights given to
long-term interactions compared to short-term ones (the vanishing gradient
problem). As a remedy to the problem, an LSTM network was proposed in 1997.&lt;/p&gt;

&lt;p&gt;An LSTM is a special kind of RNNs, and its core contribution is an introduction
of a self-connected unit (&lt;em&gt;a memory cell unit&lt;/em&gt;). The weight of this cell is
gated, as opposed to fixed, which enables the error signals to flow for long
duration. An LSTM contains three gates: a forget gate and an input gate to
control what information is stored at the memory cell unit, and an output gate
to control what information is accessed at the memory cell unit.&lt;/p&gt;

&lt;p&gt;The forget gate unit for time step $t$ takes $\bar{f}^{(t)}$ as an input
[
    \bar{f}^{(t)} = b_{f} + U_{f} \; x^{(t)} + W_{f} \; h^{(t-1)},
]
and gets $f^{(t)}$ as an activation
[
    f^{(t)} = \sigma_{f} \left( \bar{f}^{(t)} \right).
]
Here, $\sigma_{f}$ is a sigmoid, usually the logistic function;
$x^{(t)}$ is the input vector and $h^{(t)}$ is the hidden layer vector
for time $t$; and $b_{f}$, $U_{f}$, and $W_{f}$ are biases, input weights
and recurrent weights for the forget gate.&lt;/p&gt;

&lt;p&gt;The activation of the forget gate, $f^{(t)}$, can take a value between 0 and 1,
where 1 signifies &amp;ldquo;retain all of the memory cell state&amp;rdquo; and 0 signifies
&amp;ldquo;completely forget the memory cell state.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The input gate unit is computed similarly to the forget gate, but with its own
parameters. Its input is
[
    \bar{g}^{(t)} = b_{g} + U_{g} \; x^{(t)} + W_{g} \; h^{(t-1)},
]
and the output is
[
    g^{(t)} = \sigma_{g} \left( \bar{g}^{(t)} \right).
]
Here again, $\sigma_{g}$ is a sigmoid, usually the logistic function.
The activation of this input gate, $g^{(t)}$, can take a value between 0 and 1,
and determines which memory cell units are updated and how much they are
updated.&lt;/p&gt;

&lt;p&gt;The memory cell unit also gets an input from the block input unit. An input to
the block input is
[
    \bar{z}^{(t)} = b_{z} + U_{z} \; x^{(t)} + W_{z} \; h^{(t-1)},
]
and its activation is
[
    z^{(t)} = \sigma_{z} \left( \bar{z}^{(t)} \right),
]
where  $\sigma_{z}$ is usually the hyperbolic tangent function.&lt;/p&gt;

&lt;p&gt;Then, the memory cell state is updated as follows:
[
    s^{(t)} = f^{(t)} \odot s^{(t-1)} + g^{(t)} \odot z^{(t)},
]
where $\odot$ indicates element-wise (Hadamard) product.&lt;/p&gt;

&lt;p&gt;Remaining is the output gate. Its input is
[
    \bar{q}^{(t)} = b_{q} + U_{q} \; x^{(t)} + W_{q} \; h^{(t-1)},
]
and its activation is
[
    q^{(t)} = \sigma_{q} \left( \bar{q}^{(t)} \right).
]
Here again, $\sigma_{q}$ is a sigmoid, usually the logistic function.
This output gate determines which memory cell states contributes to the hidden
state and the output.&lt;/p&gt;

&lt;p&gt;Then, the hidden state is updated:
[
    h^{(t)} = \sigma_{h} \left( s^{(t)} \right) \odot q^{(t)},
]
where $\sigma_{h}$ is usually the hyperbolic tangent function. The output
from a LSTM is this hidden state vector, $h^{(t)}$.&lt;/p&gt;

&lt;p&gt;Therefore, a LSTM accepts a sequence of inputs $x^{(t)}$ and outputs a sequence
$h^{(t)}$, which I symbolically write as
[
    h = LSTM(x).
]
In practice, a LSTM is often stacked. A 4-layer LSTM network, for example, is
[
\begin{align}
    h^{(t, 1)} &amp;amp;= LSTM_{1}(x^{(t)})\\&lt;br /&gt;
    h^{(t, 2)} &amp;amp;= LSTM_{2}(h^{(t, 1)})\\&lt;br /&gt;
    h^{(t, 3)} &amp;amp;= LSTM_{3}(h^{(t, 2)})\\&lt;br /&gt;
    h^{(t, 4)} &amp;amp;= LSTM_{4}(h^{(t, 3)}).
\end{align}
]
where $h^{(t, 4)}$ is used to compute the loss function at time $t$.&lt;/p&gt;

&lt;h2 id=&#34;back-propagation-through-time&#34;&gt;Back-propagation through time&lt;/h2&gt;

&lt;p&gt;The original LSTM training algorithm (Hochreiter and Schmidhuber, 1997) used an
approximate error gradient calculated with a combination of real time recurrent
learning and back-propagation through time (BPTT). However, it is possible to
calculate the exact LSTM gradient with BPTT.&lt;/p&gt;

&lt;p&gt;For BPTT, we recursively compute gradient. Let me introduce new notations: $L$
denotes the loss function and $\tau$ is a sequence length.  Then, we derive the
gradient with respect to $h$, $s$, $\bar{q}$, $\bar{z}$, $\bar{g}$, and $\bar{f}$.
For convenience, I introduce a few notations:
[
\begin{align}
    \epsilon_{h}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial h^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{s}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial s^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{q}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{q}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{q}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{q}^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{z}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{z}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{z}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{z}^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{g}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{g}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{g}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{g}^{(t)}}
    \quad{\textrm{and}}
    \\&lt;br /&gt;
    \epsilon_{\bar{f}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{f}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{f}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{f}^{(t)}.}
\end{align}
]&lt;/p&gt;

&lt;p&gt;We first compute
[
    \epsilon_{h}^{(\tau)} = \frac{\partial L^{(\tau)}}{\partial h^{(\tau)}}
]
and
[
\begin{align}
    \epsilon_{s}^{(\tau)}
        &amp;amp;= \frac{\partial L^{(\tau)}}{\partial s^{(\tau)}}\\&lt;br /&gt;
        &amp;amp;=
            \frac{\partial L^{(\tau)}}{\partial h^{(\tau)}}
            \frac{\partial h^{(\tau)}}{\partial \sigma_{s} \left(s^{(\tau)} \right)}
            \frac{\partial \sigma_{s} \left(s^{(\tau)} \right)}{\partial s^{(\tau)}}
        \\&lt;br /&gt;
        &amp;amp;=
            \epsilon_{h}^{(\tau)}
            \odot
            q^{(\tau)}
            \odot
            \sigma_{s}&amp;rsquo;\left(s^{(\tau)} \right).
\end{align}
]&lt;/p&gt;

&lt;p&gt;Then, we go backwards through time by computing for $t = \tau - 1 , \tau - 2, \dots, 1$:
[
\begin{align}
    \epsilon_{s}^{(t)}
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial s^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \epsilon_{s}^{(t+1)} \odot f^{(t+1)}
        \\&lt;br /&gt;
        &amp;amp;= \epsilon_{h}^{(t)} \odot q^{(t)} \odot \sigma_{h}&amp;rsquo;\left(s^{(t)} \right) +
            \epsilon_{s}^{(t+1)} \odot f^{(t+1)},
\end{align}
]
and
[
\begin{align}
    \epsilon_{h}^{(t)}
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            \epsilon_{\bar{q}}^{(t+1)} \frac{\partial \bar{q}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{z}}^{(t+1)} \frac{\partial \bar{z}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{g}}^{(t+1)} \frac{\partial \bar{g}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{f}}^{(t+1)} \frac{\partial \bar{f}^{(t+1)}}{\partial h^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            W_{q}^{T} \epsilon_{\bar{q}}^{(t+1)} +
            W_{z}^{T} \epsilon_{\bar{z}}^{(t+1)} +
            W_{g}^{T} \epsilon_{\bar{g}}^{(t+1)} +
            W_{f}^{T} \epsilon_{\bar{f}}^{(t+1)},
\end{align}
]
where superscript $T$ indicates transpose.&lt;/p&gt;

&lt;p&gt;The gradient with respect to other functions is given by
[
\begin{align}
    \epsilon_{\bar{q}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t+1)}}
            \frac{\partial h^{(t+1)}}{\partial q^{(t+1)}}
            \frac{\partial q^{(t+1)}}{\partial \bar{q}^{(t+1)}}
        =
        \epsilon_{h}^{(t+1)} \odot
        \sigma_{h} \left( s^{(t+1)} \right) \odot
        \sigma_{q}&amp;rsquo; \left( \bar{q}^{(t+1)} \right),
    \\&lt;br /&gt;
    \epsilon_{\bar{z}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial z^{(t+1)}}
            \frac{\partial z^{(t+1)}}{\partial \bar{z}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        g^{(t)} \odot
        \sigma_{z}&amp;rsquo; \left( \bar{z}^{(t+1)} \right),
    \\&lt;br /&gt;
    \epsilon_{\bar{g}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial g^{(t+1)}}
            \frac{\partial g^{(t+1)}}{\partial \bar{g}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        z^{(t)} \odot
        \sigma_{g}&amp;rsquo; \left( \bar{g}^{(t+1)} \right), \textrm{ and }
    \\&lt;br /&gt;
    \epsilon_{\bar{f}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial f^{(t+1)}}
            \frac{\partial f^{(t+1)}}{\partial \bar{f}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        s^{(t)} \odot
        \sigma_{f}&amp;rsquo; \left( \bar{f}^{(t+1)} \right).
\end{align}
]&lt;/p&gt;

&lt;p&gt;The gradient for each parameter is computed with the above terms. For example,
[
    \frac{\partial L}{\partial W_{f}}
        = \sum_{t=1}^{\tau} \frac{\partial L}{\partial \bar{f}^{(t)}} \frac{\partial \bar{f}^{(t)}}{\partial W_{f}}
        = \sum_{t=1}^{\tau} \epsilon_{\bar{f}}^{(t)} \otimes x^{(t)},
]
where $\otimes$ indicates outer product.&lt;/p&gt;

&lt;p&gt;For a multi-layer LSTM, we need to compute the gradient with respect to the
input $x$.
[
\begin{align}
    \frac{\partial L}{\partial x^{(t)}}
    &amp;amp;=
        \frac{\partial L}{\partial \bar{f}^{(t)}}
        \frac{\partial \bar{f}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{g}^{(t)}}
        \frac{\partial \bar{g}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{z}^{(i)}}
        \frac{\partial \bar{z}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{q}^{(t)}}
        \frac{\partial \bar{q}^{(t)}}{\partial x^{(t)}}
    \\&lt;br /&gt;
    &amp;amp;=
        W_{f}^{T} \, \epsilon_{\bar{f}}^{(t)}
        +
        W_{g}^{T} \, \epsilon_{\bar{g}}^{(t)}
        +
        W_{z}^{T} \, \epsilon_{\bar{z}}^{(t)}
        +
        W_{q}^{T} \, \epsilon_{\bar{q}}^{(t)}
\end{align}
]&lt;/p&gt;

&lt;h2 id=&#34;references-and-bibliography&#34;&gt;References and Bibliography&lt;/h2&gt;

&lt;p&gt;Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. &lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;link to
the book website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Graves, A. (2012). Supervised sequence labelling with recurrent neural
networks. Springer. &lt;a href=&#34;http://www.cs.toronto.edu/~graves/preprint.pdf&#34;&gt;link to preprint
pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., and Schmidhuber,
J. (2015). LSTM: A Search Space Odyssey. &lt;a href=&#34;https://arxiv.org/abs/1503.04069&#34;&gt;arXiv:1503.04069
(cs.NE)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. &lt;em&gt;Neural
computation, 9,&lt;/em&gt; 1735-1780.&lt;/p&gt;

&lt;p&gt;Jimenez, N. D.  (2014). Simple LSTM. &lt;a href=&#34;http://nicodjimenez.github.io/2014/08/08/lstm.html&#34;&gt;link to the blog
post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Olah, C. (2015). Understanding LSTM networks. &lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;link to the blog
post&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hidden Markov Model</title>
      <link>https://tkngch.github.io/post/2017-01-15_Hidden-Markov-Model/</link>
      <pubDate>Sun, 15 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-01-15_Hidden-Markov-Model/</guid>
      <description>&lt;p&gt;Deep learning is quite popular in sequence modelling, but this blog post
discusses a more traditional model, a hidden Markov model.&lt;/p&gt;

&lt;p&gt;(Updated on 18 March 2017)&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;A hidden Markov model (HMM) is a model of stochastic time-series: it models a
sequence of outputs $y_{t}, \, \left( t = 1, 2, \dots, T \right)$. These outputs are
conditioned on a sequence of latent states $z_{t} = 1, 2, \dots, K$.
These &amp;ldquo;hidden&amp;rdquo; state variables are formalised with a Markov chain such that
[
    p(z_{t} \vert z_{t-1}, z_{t-2}, z_{t-3} \dots) = p(z_{t} \vert z_{t-1}).
]
This Markov chain is parameterised by a $K \times K$ transition matrix $A$.
Here, the probability of transitioning to state $z_{t}$ from state $z_{t-1}$
is given by $A_{z_{t}, z_{t-1}}$.&lt;/p&gt;

&lt;p&gt;For this Markov process, we need to define the probability of the initial
states, which is often denoted as $\pi$:
[
    \pi_{i} = p(z_{1} = i).
]&lt;/p&gt;

&lt;p&gt;The output $y_{t}$ at time $t$ is generated based on latent state $z_{t}$:
[
    p(y_{t} \vert z_{t}) = f(y_{t} \vert \theta_{z_{t}}),
]
where $\theta_{i}$ is a set of parameters to characterise latent state $i$, and
the emission probability function, $f$, specifies how the output was generated.
$f$ is often probability mass function of multinoulli distribution or
probability density function of Gaussian distribution.&lt;/p&gt;

&lt;p&gt;Therefore, HMMs require specification of three parameters, $A$, $\pi$, and
$\theta$, to model the observed data. For convenience, I use the compact
notation:
[
\begin{align}
    Y &amp;amp;= \{y_{1}, \, y_{2}, \, \dots, y_{T}\},\\&lt;br /&gt;
    Z &amp;amp;= \{z_{1}, \, z_{2}, \, \dots, z_{T}\}, \quad{and}\\&lt;br /&gt;
    \lambda &amp;amp;= \{A, \, \pi, \, \theta\}.
\end{align}
]&lt;/p&gt;

&lt;p&gt;In practice, HMMs have to solve two problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Given the model parameters, what is the most likely sequence of latent states?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Given the observed data, what is the most likely parameter values?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To solve these problems, we need to know how to calculate the likelihood of the
observed data given the parameters.&lt;/p&gt;

&lt;h2 id=&#34;likelihood-computation&#34;&gt;Likelihood Computation&lt;/h2&gt;

&lt;p&gt;A naive approach to compute $p(Y \vert \lambda)$ is to simply integrate out $Z$:
[
\begin{align}
    p(Y \vert \lambda)
        &amp;amp;= \int p(Y, Z \vert \lambda) \, d_{Z}\\&lt;br /&gt;
        &amp;amp;= \int p(Y \vert Z, \, \lambda) \, p(Z \vert \lambda) \, d_{Z}\\&lt;br /&gt;
        &amp;amp;= \int \left( \prod_{t=1}^{T} f(y_{t} \vert \theta_{z_{t}}) \right)
        \left( p(z_{1}) \, \prod_{t=2}^{T} A_{z_{t-1}, z_{t}} \right) d_{Z}.
\end{align}
]
This integration over $Z$ may not be feasible, as the above equation involves on
the order of $2TK^{T}$ calculations. This is because at every $t = 1, 2,
\dots, T$, there are $K$ possible latent states, resulting in $K^{T}$
possible state sequence. In addition, for each state sequence, about $2T$
calculations are required.&lt;/p&gt;

&lt;p&gt;Fortunately, a more efficient procedure exists, which is called
&lt;em&gt;forward algorithm&lt;/em&gt;. We first define the forward variable,
[
    \alpha_{t}(i) = p(y_{1}, y_{2}, \dots, y_{t}, z_{t} = i \vert \lambda),
]
which is the probability of the partial observation sequence up to time $t$ and
latent state $i$ at time $t$.&lt;/p&gt;

&lt;p&gt;With this forward variable, the likelihood can be computed as
[
    p(Y \vert \lambda) = \sum_{i=1}^{K} \alpha_{T}(i),
]
where the forward variable is computed recursively:
[
    \alpha_{t}(i) =
        \begin{cases}
            \pi_{i} \, f(y_{t} \vert \theta_{i})
            &amp;amp; \textrm{for } t = 1
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \left(
                \sum_{j = 1}^{K} \alpha_{t-1}(j) \, A_{i, j}
            \right)
            f(y_{t} \vert \theta_{i})
            &amp;amp; \textrm{for } t = 2, 3, \dots, T.
        \end{cases}
]
As we need to calculate $\alpha_{t}(i)$ for each $i = 1, 2, \dots, K$,
the forward variable requires the order of $K^{2} T$ calculations, much less
than our naive approach.&lt;/p&gt;

&lt;p&gt;The above computation of forward variable can be expressed as matrix
multiplication. We assume $\alpha_{t}$ and $\pi$ are column vectors, and
define $F$ as a $K \times K$ diagonal matrix with $f(y_{t} \vert
\theta_{i})$:
[
    F_{t} = \begin{bmatrix}
        f(y_{t} \vert \theta_{1}) &amp;amp; 0 &amp;amp; 0 &amp;amp; \dots &amp;amp; 0\\&lt;br /&gt;
        0 &amp;amp; f(y_{t} \vert \theta_{2}) &amp;amp; 0 &amp;amp; \dots &amp;amp; 0\\&lt;br /&gt;
        0 &amp;amp; 0 &amp;amp; f(y_{t} \vert \theta_{3}) &amp;amp; \dots &amp;amp; 0\\&lt;br /&gt;
        0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \dots &amp;amp; f(y_{t} \vert \theta_{K})\\&lt;br /&gt;
    \end{bmatrix}.
]
Then,
[
    \alpha_{t} =
        \begin{cases}
            F_{t} \, \pi
            &amp;amp; \textrm{for } t = 1
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            F_{t} \, A \, \alpha_{t-1}
            &amp;amp; \textrm{for } t = 2, 3, \dots, T.
        \end{cases}
]&lt;/p&gt;

&lt;h2 id=&#34;finding-the-sequence-of-latent-states-the-first-approach&#34;&gt;Finding the sequence of latent states: the first approach&lt;/h2&gt;

&lt;p&gt;Now we consider how to find the most likely sequence of latent states, given
the parameters. One simpler approach is to maximise the probability of latent
state at each time point:
[
    z_{t}^{*} = \arg \max_{i} p(z_{t} = i \vert Y, \, \lambda).
]&lt;/p&gt;

&lt;p&gt;To compute this probability, we define the backward variable,
[
    \beta_{t}(i) = p(y_{t+1}, y_{t+2}, \dots, y_{T} \vert z_{t} = i, \, \lambda),
]
which is the probability of the partial observation sequence from $t+1$, given
latent state $i$ at time $t$.  As with the forward variable, the backward
variable can be computed recursively
[
    \beta_{t}(i) =
        \begin{cases}
            \sum_{j = 1}^{K} A_{i, j} \, \beta_{t+1}(j) \, f(y_{t+1} \vert \theta_{j})
            &amp;amp; \textrm{for } t = {1, 2, \dots, T - 1}
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            1
            &amp;amp; \textrm{for } t = T.
        \end{cases}
]&lt;/p&gt;

&lt;p&gt;The above computation can be again expressed as matrix multiplications.
Assuming that $\beta_{t}$ is a column vector,
[
    \beta_{t} =
        \begin{cases}
            F_{t} \, A&amp;rsquo; \, \beta_{t+1}
            &amp;amp; \textrm{for } t = {1, 2, \dots, T - 1}
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \mathbf{1}
            &amp;amp; \textrm{for } t = T,
        \end{cases}
]
where $A&amp;rsquo;$ is a transposition of $A$ and $\mathbf{1}$ is a column vector of ones.&lt;/p&gt;

&lt;p&gt;Then, the probability of latent state $i$ at time $t$ is given by
[
\begin{align}
    p(z_{t} = i \vert Y, \, \lambda) &amp;amp;=
        \frac{
            p(y_{1}, y_{2}, \dots, y_{t}, z_{t} = i \vert \lambda)
            \,
            p(y_{t+1}, y_{t+2}, \dots, y_{T} \vert z_{t} = i, \, \lambda)
        }{
            p(Y \vert \lambda)
        }
        \\&lt;br /&gt;
        &amp;amp;=
        \frac{
            \alpha_{t}(i)
            \,
            \beta_{t}(i)
        }{
            p(Y \vert \lambda).
        }
\tag{1}\label{eq:gamma}
\end{align}
]
The denominator does not depend on $z_{t}$, so we only need to consider
[
    p(z_{t} = i \vert Y, \, \lambda) \propto \alpha_{t}(i) \, \beta_{t}(i),
]
and the most likely state at time $t$ is
[
    \arg \max_{i} \alpha_{t}(i) \, \beta_{t}(i).
]&lt;/p&gt;

&lt;p&gt;This simpler approach considers the most likely state at each time point
independently, and thus, the resulting sequence can be very unlikely. For
example, suppose the approach finds that the latent state is $1$ at time $t$
and $2$ at time $t + 1$. But this estimation does not consider $A_{1, 2}$ and
is impossible when $A_{1,2} = 0$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Viterbi algorithm&lt;/em&gt;, we discuss next, considers multiple time points to find
the sequence of latent states that maximise $p(Z \vert y, \, \lambda)$.&lt;/p&gt;

&lt;h2 id=&#34;finding-the-sequence-of-latent-states-viterbi-algorithm&#34;&gt;Finding the sequence of latent states: Viterbi algorithm&lt;/h2&gt;

&lt;p&gt;With the Viterbi algorithm, we sequentially compute the probability of the most
probable state sequence which ends in latent state $i$ at time $t$,
[
    \delta_{t}(i) = \max_{z_{1}, z_{2}, \dots, z_{t - 1}}
        p(z_{1}, z_{2}, \dots, z_{t - 1}, z_{t} = i, y_{1}, y_{2}, \dots, y_{t} \vert \lambda).
]
This probability can be computed recursively
[
    \delta_{t}(i) =
        \begin{cases}
            \pi_{i} \, f(y_{t} \vert \theta_{i})
            &amp;amp; \textrm{for } t = 1
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \max_{j} \left( \delta_{t-1}(j) \, A_{j, i} \right)
                f(y_{t} \vert \theta_{j})
            &amp;amp; \textrm{for } t = {2, 3, \dots, T}.
        \end{cases}
]&lt;/p&gt;

&lt;p&gt;When computing $\delta$, we store the state used at each time $t$:
[
    \psi_{t}(i) =
        \begin{cases}
            0
            &amp;amp; \textrm{for } t = 1
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \arg \max_{j} \left( \delta_{t-1}(j) \, A_{j, i} \right)
            &amp;amp; \textrm{for } t = {2, 3, \dots, T}.
        \end{cases}
]
Then, the best state sequence can be found by backtracking,
[
    z_{t}^{*} =
        \begin{cases}
            \arg \max_{j} \delta_{t-1}(j) &amp;amp; \textrm{for } t = T
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \psi_{t+1}(z_{t+1}^{*}) &amp;amp; \textrm{for } t = T - 1, T - 2, \dots, 1.
        \end{cases}
]
This backtracking first identifies the latent state where the most probable
sequence ends up, $z_{T}^{*}$, and finds which latent state is most
likely preceded. Then it repeats this backtracking till the beginning of the
sequence.&lt;/p&gt;

&lt;h2 id=&#34;parameter-values&#34;&gt;Parameter values&lt;/h2&gt;

&lt;p&gt;There are several procedures to estimate parameters, but here, I review the
Baum-Welch algorithm, which uses the EM algorithm to find the maximum
likelihood estimate of the parameters.&lt;/p&gt;

&lt;p&gt;In order to describe the algorithm, I define the probability of being in the
latent state $i$ at time $t$ and latent state $j$ at time $t+1$:
[
    \xi_{t}(i, j) = p(z_{t} = i,\, z_{t+1} = j \vert Y, \lambda).
]
Using the forward and backward variables,
[
\begin{align}
    \xi_{t}(i, j)
    &amp;amp;= \frac{
            p(z_{t} = i,\, z_{t+1} = j, \, Y \vert \lambda)
        }{
            p(Y \vert \lambda)
        }
    \\&lt;br /&gt;
    &amp;amp;= \frac{
        p(y_{1}, y_{2}, \dots, y_{t}, z_{t} = i \vert \lambda)
        \,
        p(z_{t+1} = j \vert z_{t} = i)
        \,
        p(y_{t+1} \vert z_{t+1} = j, \lambda)
        \,
        p(y_{t+2}, y_{t+3}, \dots, y_{T} \vert z_{t+1} = j, \, \lambda)
        }{
            p(Y \vert \lambda)
        }
    \\&lt;br /&gt;
    &amp;amp;= \frac{
            \alpha_{t}(i) \, A_{j,i} \, f(y_{t+1} \vert \theta_{j}) \, \beta_{t+1}(j)
        }{
            p(Y \vert \lambda)
        }
    \\&lt;br /&gt;
    &amp;amp;= \frac{
            \alpha_{t}(i) \, A_{j,i} \, f(y_{t+1} \vert \theta_{j}) \, \beta_{t+1}(j)
        }{
            \sum_{i=1}^{K}
            \sum_{j=1}^{K}
            \alpha_{t}(i) \, A_{j,i} \, f(y_{t+1} \vert \theta_{j}) \, \beta_{t+1}(j).
        }
\end{align}
]
Then, the probability of being in latent state $i$ at time $t$ is defined as
[
    \gamma_{t}(i) = \sum_{j=1}^{K} \xi_{t}(i, j).
]
Note that this $\gamma$ is a redifinition of Equation \ref{eq:gamma}: that is,
[
    \gamma_{t}(i)
        = p(z_{t} = i \vert Y, \, \lambda)
        =
        \frac{
            \alpha_{t}(i)
            \,
            \beta_{t}(i)
        }{
            p(Y \vert \lambda)
        }
]&lt;/p&gt;

&lt;p&gt;If we sum $\gamma_{t}(i)$ from $t = 1$ to $t = T -1$, we get the expected
number of transitions from latent state $i$. If we sum $\xi_{t}(i,j)$ over
$t$, we get the expected number of transitions from latent state $i$ to latent
state $j$.&lt;/p&gt;

&lt;p&gt;Now we can update the parameters:
[
\begin{align}
    \pi_{i} &amp;amp;\gets \gamma_{1}(i)
    \\&lt;br /&gt;
    a_{ij} &amp;amp;\gets \frac{
        \sum_{t=1}^{T-1} \xi_{t}(i, j)
        }{
        \sum_{t=1}^{T-1} \gamma_{t}(i)
    }.
\end{align}
]&lt;/p&gt;

&lt;p&gt;Similarly, $\theta$ can be updated. If $f$ is the probability mass function for
multinoulli distribution,
[
    \theta_{i,v} = p(y = v \vert z = i)
    \gets \frac{
        \sum_{t=1}^{T-1} \mathbb{1}_{y_{t} = v} \gamma_{t}(i)
        }{
        \sum_{t=1}^{T-1} \gamma_{t}(i)
    }.
]
Here, $\mathbb{1}$ is the indicator function.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;HMMs are implemented in
&lt;a href=&#34;http://scikit-learn.sourceforge.net/stable/modules/hmm.html&#34;&gt;scikit-learn&lt;/a&gt;,
but this scikit-learn module has been moved to a separate module
&lt;a href=&#34;https://github.com/hmmlearn/hmmlearn&#34;&gt;hmmlearn&lt;/a&gt;.  There are probably more out
there.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;Rabiner, L. R. (1998). A tutorial on hidden Markov models and selected
applications in speech recognition. &lt;em&gt;Proceedings of The IEEE, 77,&lt;/em&gt; 257-286.
&lt;a href=&#34;http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial
on hmm and applications.pdf&#34;&gt;link to pdf&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>