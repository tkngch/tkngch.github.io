<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Not Much of a Blog</title>
    <link>https://tkngch.github.io/</link>
    <description>Recent content on Not Much of a Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2016 Takao Noguchi. Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Sun, 05 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://tkngch.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Joint probability as a tensor</title>
      <link>https://tkngch.github.io/post/2018-08-05_joint-probability-as-tensor/</link>
      <pubDate>Sun, 05 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2018-08-05_joint-probability-as-tensor/</guid>
      <description>&lt;p&gt;In this post, I consider joint probability of discrete random variables as tensors.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Gradient Descent by Gradient Descent</title>
      <link>https://tkngch.github.io/post/2017-02-19_gradient-descent-by-gradient-descent/</link>
      <pubDate>Sun, 19 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-02-19_gradient-descent-by-gradient-descent/</guid>
      <description>&lt;p&gt;In this blog post, I review the 2016 NIPS paper &amp;ldquo;learning to learn by gradient
descent by gradient descent&amp;rdquo; (I abbreviate as LLGG) by Andrychowicz et al.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory Network and Back-Propagation through Time</title>
      <link>https://tkngch.github.io/post/2017-01-29_long-short-term-memory-network/</link>
      <pubDate>Sun, 29 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-01-29_long-short-term-memory-network/</guid>
      <description>&lt;p&gt;Recurrent neural networks (RNNs) are neural networks to model sequential data.
RNNs are often used in speech recognition and natural language processing. In
this blog post, I discuss one of the most popular RNNs, a long short-term
memory (LSTM) network. Then I briefly address a training procedure for a LSTM.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hidden Markov Model</title>
      <link>https://tkngch.github.io/post/2017-01-15_hidden-markov-model/</link>
      <pubDate>Sun, 15 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-01-15_hidden-markov-model/</guid>
      <description>&lt;p&gt;Deep learning is quite popular in sequence modelling, but this blog post
discusses a more traditional model, a hidden Markov model.&lt;/p&gt;

&lt;p&gt;(Updated on 18 March 2017)&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hierarchical Bayes</title>
      <link>https://tkngch.github.io/post/2016-12-26_hierarchical-bayes/</link>
      <pubDate>Mon, 26 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2016-12-26_hierarchical-bayes/</guid>
      <description>&lt;p&gt;Sometimes, data contain multiple entries from observation units. For example,
researchers may be interested in effectiveness of medical treatments, and
tested several treatments in several different cities. Then, it may make sense
to consider that the effectiveness may be overall consistent across cities but
slightly vary between the cities, as population characteristic may vary between
the cities. For instance, one city may have more younger people than another.
In these cases, hierarchical Bayes may help. In this blog post, I review
hierarchical Bayes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dirichlet process mixture model as a cognitive model of category learning</title>
      <link>https://tkngch.github.io/post/2016-12-17/</link>
      <pubDate>Sat, 17 Dec 2016 15:30:10 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2016-12-17/</guid>
      <description>&lt;p&gt;In cognitive science/psychology, Dirichlet process mixture model (DPMM) is
considered as a rational model of category learning (Anderson, 1991; Sanborn,
Griffiths &amp;amp; Navarro, 2010). That is, the DPMM is used to approximate how human
learns to categorise objects.&lt;/p&gt;

&lt;p&gt;In this post, I review the DPMM, assuming that all the features are discrete.
The implementation in C++ can be found
&lt;a href=&#34;https://github.com/tkngch/dirichlet-process-mixture-model&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>