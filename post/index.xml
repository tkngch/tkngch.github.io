<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Not Much of a Blog</title>
    <link>https://tkngch.github.io/post/index.xml</link>
    <description>Recent content in Posts on Not Much of a Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>&amp;copy; 2016 Takao Noguchi. Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Sun, 29 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://tkngch.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Long Short-Term Memory Network and Back-Propagation through Time</title>
      <link>https://tkngch.github.io/post/2017-01-29_Long-Short-Term-Memory-Network/</link>
      <pubDate>Sun, 29 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-01-29_Long-Short-Term-Memory-Network/</guid>
      <description>&lt;p&gt;Recurrent neural networks (RNNs) are neural networks to model sequential data.
RNNs are often used in speech recognition and natural language processing. In
this blog post, I discuss one of the most popular RNNs, a long short-term
memory (LSTM) network. Then I briefly address a training procedure for a LSTM.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;long-short-term-memory-network&#34;&gt;Long short-term memory network&lt;/h2&gt;

&lt;p&gt;While RNNs in theory can use contextual information to map input to output
sequences, RNNs in practice can use only a limited range of context. The basic
problem is that error signals propagated through time tend to either vanish or
explode. This difficulty arises from the exponentially smaller weights given to
long-term interactions compared to short-term ones (the vanishing gradient
problem). As a remedy to the problem, an LSTM network was proposed in 1997.&lt;/p&gt;

&lt;p&gt;An LSTM is a special kind of RNNs, and its core contribution is an introduction
of a self-connected unit (&lt;em&gt;a memory cell unit&lt;/em&gt;). The weight of this cell is
gated, as opposed to fixed, which enables the error signals to flow for long
duration. An LSTM contains three gates: a forget gate and an input gate to
control what information is stored at the memory cell unit, and an output gate
to control what information is accessed at the memory cell unit.&lt;/p&gt;

&lt;p&gt;The forget gate unit for time step $t$ takes $\bar{f}^{(t)}$ as an input
[
    \bar{f}^{(t)} = b_{f} + U_{f} \; x^{(t)} + W_{f} \; h^{(t-1)},
]
and gets $f^{(t)}$ as an activation
[
    f^{(t)} = \sigma_{f} \left( \bar{f}^{(t)} \right).
]
Here, $\sigma_{f}$ is a sigmoid, usually the logistic function;
$x^{(t)}$ is the input vector and $h^{(t)}$ is the hidden layer vector
for time $t$; and $b_{f}$, $U_{f}$, and $W_{f}$ are biases, input weights
and recurrent weights for the forget gate.&lt;/p&gt;

&lt;p&gt;The activation of the forget gate, $f^{(t)}$, can take a value between 0 and 1,
where 1 signifies &amp;ldquo;retain all of the memory cell state&amp;rdquo; and 0 signifies
&amp;ldquo;completely forget the memory cell state.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The input gate unit is computed similarly to the forget gate, but with its own
parameters. Its input is
[
    \bar{g}^{(t)} = b_{g} + U_{g} \; x^{(t)} + W_{g} \; h^{(t-1)},
]
and the output is
[
    g^{(t)} = \sigma_{g} \left( \bar{g}^{(t)} \right).
]
Here again, $\sigma_{g}$ is a sigmoid, usually the logistic function.
The activation of this input gate, $g^{(t)}$, can take a value between 0 and 1,
and determines which memory cell units are updated and how much they are
updated.&lt;/p&gt;

&lt;p&gt;The memory cell unit also gets an input from the block input unit. An input to
the block input is
[
    \bar{z}^{(t)} = b_{z} + U_{z} \; x^{(t)} + W_{z} \; h^{(t-1)},
]
and its activation is
[
    z^{(t)} = \sigma_{z} \left( \bar{z}^{(t)} \right),
]
where  $\sigma_{z}$ is usually the hyperbolic tangent function.&lt;/p&gt;

&lt;p&gt;Then, the memory cell state is updated as follows:
[
    s^{(t)} = f^{(t)} \odot s^{(t-1)} + g^{(t)} \odot z^{(t)},
]
where $\odot$ indicates element-wise (Hadamard) product.&lt;/p&gt;

&lt;p&gt;Remaining is the output gate. Its input is
[
    \bar{q}^{(t)} = b_{q} + U_{q} \; x^{(t)} + W_{q} \; h^{(t-1)},
]
and its activation is
[
    q^{(t)} = \sigma_{q} \left( \bar{q}^{(t)} \right).
]
Here again, $\sigma_{q}$ is a sigmoid, usually the logistic function.
This output gate determines which memory cell states contributes to the hidden
state and the output.&lt;/p&gt;

&lt;p&gt;Then, the hidden state is updated:
[
    h^{(t)} = \sigma_{h} \left( s^{(t)} \right) \odot q^{(t)},
]
where $\sigma_{h}$ is usually the hyperbolic tangent function. The output
from a LSTM is this hidden state vector, $h^{(t)}$.&lt;/p&gt;

&lt;p&gt;Therefore, a LSTM accepts a sequence of inputs $x^{(t)}$ and outputs a sequence
$h^{(t)}$, which I symbolically write as
[
    h = LSTM(x).
]
In practice, a LSTM is often stacked. A 4-layer LSTM network, for example, is
[
\begin{align}
    h^{(t, 1)} &amp;amp;= LSTM_{1}(x^{(t)})\\&lt;br /&gt;
    h^{(t, 2)} &amp;amp;= LSTM_{2}(h^{(t, 1)})\\&lt;br /&gt;
    h^{(t, 3)} &amp;amp;= LSTM_{3}(h^{(t, 2)})\\&lt;br /&gt;
    h^{(t, 4)} &amp;amp;= LSTM_{4}(h^{(t, 3)}).
\end{align}
]
where $h^{(t, 4)}$ is used to compute the loss function at time $t$.&lt;/p&gt;

&lt;h2 id=&#34;back-propagation-through-time&#34;&gt;Back-propagation through time&lt;/h2&gt;

&lt;p&gt;The original LSTM training algorithm (Hochreiter and Schmidhuber, 1997) used an
approximate error gradient calculated with a combination of real time recurrent
learning and back-propagation through time (BPTT). However, it is possible to
calculate the exact LSTM gradient with BPTT.&lt;/p&gt;

&lt;p&gt;For BPTT, we recursively compute gradient. Let me introduce new notations: $L$
denotes the loss function and $\tau$ is a sequence length.  Then, we derive the
gradient with respect to $h$, $s$, $\bar{q}$, $\bar{z}$, $\bar{g}$, and $\bar{f}$.
For convenience, I introduce a few notations:
[
\begin{align}
    \epsilon_{h}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial h^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{s}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial s^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{q}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{q}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{q}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{q}^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{z}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{z}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{z}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{z}^{(t)}}
    \\&lt;br /&gt;
    \epsilon_{\bar{g}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{g}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{g}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{g}^{(t)}}
    \quad{\textrm{and}}
    \\&lt;br /&gt;
    \epsilon_{\bar{f}}^{(t)}
        &amp;amp;= \frac{\partial L}{\partial \bar{f}^{(t)}}
        = \sum_{i=1}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{f}^{(t)}}
        = \sum_{i=t}^{\tau} \frac{\partial L^{(i)}}{\partial \bar{f}^{(t)}.}
\end{align}
]&lt;/p&gt;

&lt;p&gt;We first compute
[
    \epsilon_{h}^{(\tau)} = \frac{\partial L^{(\tau)}}{\partial h^{(\tau)}}
]
and
[
\begin{align}
    \epsilon_{s}^{(\tau)}
        &amp;amp;= \frac{\partial L^{(\tau)}}{\partial s^{(\tau)}}\\&lt;br /&gt;
        &amp;amp;=
            \frac{\partial L^{(\tau)}}{\partial h^{(\tau)}}
            \frac{\partial h^{(\tau)}}{\partial \sigma_{s} \left(s^{(\tau)} \right)}
            \frac{\partial \sigma_{s} \left(s^{(\tau)} \right)}{\partial s^{(\tau)}}
        \\&lt;br /&gt;
        &amp;amp;=
            \epsilon_{h}^{(\tau)}
            \odot
            q^{(\tau)}
            \odot
            \sigma_{s}&amp;rsquo;\left(s^{(\tau)} \right).
\end{align}
]&lt;/p&gt;

&lt;p&gt;Then, we go backwards through time by computing for $t = \tau - 1 , \tau - 2, \dots, 1$:
[
\begin{align}
    \epsilon_{s}^{(t)}
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial s^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(i)}}{\partial s^{(t)}} +
            \epsilon_{s}^{(t+1)} \odot f^{(t+1)}
        \\&lt;br /&gt;
        &amp;amp;= \epsilon_{h}^{(t)} \odot q^{(t)} \odot \sigma_{h}&amp;rsquo;\left(s^{(t)} \right) +
            \epsilon_{s}^{(t+1)} \odot f^{(t+1)},
\end{align}
]
and
[
\begin{align}
    \epsilon_{h}^{(t)}
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            \epsilon_{\bar{q}}^{(t+1)} \frac{\partial \bar{q}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{z}}^{(t+1)} \frac{\partial \bar{z}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{g}}^{(t+1)} \frac{\partial \bar{g}^{(t+1)}}{\partial h^{(t)}} +
            \epsilon_{\bar{f}}^{(t+1)} \frac{\partial \bar{f}^{(t+1)}}{\partial h^{(t)}}
        \\&lt;br /&gt;
        &amp;amp;= \frac{\partial L^{(t)}}{\partial h^{(t)}} +
            W_{q}^{T} \epsilon_{\bar{q}}^{(t+1)} +
            W_{z}^{T} \epsilon_{\bar{z}}^{(t+1)} +
            W_{g}^{T} \epsilon_{\bar{g}}^{(t+1)} +
            W_{f}^{T} \epsilon_{\bar{f}}^{(t+1)},
\end{align}
]
where superscript $T$ indicates transpose.&lt;/p&gt;

&lt;p&gt;The gradient with respect to other functions is given by
[
\begin{align}
    \epsilon_{\bar{q}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial h^{(t+1)}}
            \frac{\partial h^{(t+1)}}{\partial q^{(t+1)}}
            \frac{\partial q^{(t+1)}}{\partial \bar{q}^{(t+1)}}
        =
        \epsilon_{h}^{(t+1)} \odot
        \sigma_{h} \left( s^{(t+1)} \right) \odot
        \sigma_{q}&amp;rsquo; \left( \bar{q}^{(t+1)} \right),
    \\&lt;br /&gt;
    \epsilon_{\bar{z}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial z^{(t+1)}}
            \frac{\partial z^{(t+1)}}{\partial \bar{z}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        g^{(t)} \odot
        \sigma_{z}&amp;rsquo; \left( \bar{z}^{(t+1)} \right),
    \\&lt;br /&gt;
    \epsilon_{\bar{g}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial g^{(t+1)}}
            \frac{\partial g^{(t+1)}}{\partial \bar{g}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        z^{(t)} \odot
        \sigma_{g}&amp;rsquo; \left( \bar{g}^{(t+1)} \right), \textrm{ and }
    \\&lt;br /&gt;
    \epsilon_{\bar{f}}^{(t+1)}
    &amp;amp;=
        \sum_{i=t+1}^{\tau} \frac{\partial L^{(i)}}{\partial s^{(t+1)}}
            \frac{\partial s^{(t+1)}}{\partial f^{(t+1)}}
            \frac{\partial f^{(t+1)}}{\partial \bar{f}^{(t+1)}}
        =
        \epsilon_{s}^{(t+1)} \odot
        s^{(t)} \odot
        \sigma_{f}&amp;rsquo; \left( \bar{f}^{(t+1)} \right).
\end{align}
]&lt;/p&gt;

&lt;p&gt;The gradient for each parameter is computed with the above terms. For example,
[
    \frac{\partial L}{\partial W_{f}}
        = \sum_{t=1}^{\tau} \frac{\partial L}{\partial \bar{f}^{(t)}} \frac{\partial \bar{f}^{(t)}}{\partial W_{f}}
        = \sum_{t=1}^{\tau} \epsilon_{\bar{f}}^{(t)} \otimes x^{(t)},
]
where $\otimes$ indicates outer product.&lt;/p&gt;

&lt;p&gt;For a multi-layer LSTM, we need to compute the gradient with respect to the
input $x$.
[
\begin{align}
    \frac{\partial L}{\partial x^{(t)}}
    &amp;amp;=
        \frac{\partial L}{\partial \bar{f}^{(t)}}
        \frac{\partial \bar{f}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{g}^{(t)}}
        \frac{\partial \bar{g}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{z}^{(i)}}
        \frac{\partial \bar{z}^{(t)}}{\partial x^{(t)}}
        +
        \frac{\partial L}{\partial \bar{q}^{(t)}}
        \frac{\partial \bar{q}^{(t)}}{\partial x^{(t)}}
    \\&lt;br /&gt;
    &amp;amp;=
        W_{f}^{T} \, \epsilon_{\bar{f}}^{(t)}
        +
        W_{g}^{T} \, \epsilon_{\bar{g}}^{(t)}
        +
        W_{z}^{T} \, \epsilon_{\bar{z}}^{(t)}
        +
        W_{q}^{T} \, \epsilon_{\bar{q}}^{(t)}
\end{align}
]&lt;/p&gt;

&lt;h2 id=&#34;references-and-bibliography&#34;&gt;References and Bibliography&lt;/h2&gt;

&lt;p&gt;Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. &lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;link to
the book website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Graves, A. (2012). Supervised sequence labelling with recurrent neural
networks. Springer. &lt;a href=&#34;http://www.cs.toronto.edu/~graves/preprint.pdf&#34;&gt;link to preprint
pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., and Schmidhuber,
J. (2015). LSTM: A Search Space Odyssey. &lt;a href=&#34;https://arxiv.org/abs/1503.04069&#34;&gt;arXiv:1503.04069
(cs.NE)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. &lt;em&gt;Neural
computation, 9,&lt;/em&gt; 1735-1780.&lt;/p&gt;

&lt;p&gt;Jimenez, N. D.  (2014). Simple LSTM. &lt;a href=&#34;http://nicodjimenez.github.io/2014/08/08/lstm.html&#34;&gt;link to the blog
post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Olah, C. (2015). Understanding LSTM networks. &lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;link to the blog
post&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hidden Markov Model</title>
      <link>https://tkngch.github.io/post/2017-01-15_Hidden-Markov-Model/</link>
      <pubDate>Sun, 15 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2017-01-15_Hidden-Markov-Model/</guid>
      <description>&lt;p&gt;Deep learning is quite popular in sequence modelling, but this blog post
discusses a more traditional model, a hidden Markov model.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;A hidden Markov model (HMM) is a model of stochastic time-series: it models a
sequence of outputs $y_{t}, \, \left( t = 1, 2, \dots, T \right)$. These outputs are
conditioned on a sequence of latent states $z_{t} = 1, 2, \dots, K$.
These &amp;ldquo;hidden&amp;rdquo; state variables are formalised with a Markov chain such that
[
    p(z_{t} \vert z_{t-1}, z_{t-2}, z_{t-3} \dots) = p(z_{t} \vert z_{t-1}).
]
This Markov chain is parameterised by a $K \times K$ transition matrix $A$.
Here, the probability of transitioning to state $z_{t}$ from state $z_{t-1}$
is given by $A_{z_{t}, z_{t-1}}$.&lt;/p&gt;

&lt;p&gt;For this Markov process, we need to define the probability of the initial
states, which is often denoted as $\pi$:
[
    \pi_{i} = p(z_{1} = i).
]&lt;/p&gt;

&lt;p&gt;The output $y_{t}$ at time $t$ is generated based on latent state $z_{t}$:
[
    p(y_{t} \vert z_{t}) = f(y_{t} \vert \theta_{z_{t}}),
]
where $\theta_{i}$ is a set of parameters to characterise latent state $i$, and
the emission probability function, $f$, specifies how the output was generated.
$f$ is often probability mass function of multinoulli distribution or
probability density function of Gaussian distribution.&lt;/p&gt;

&lt;p&gt;Therefore, HMMs require specification of three parameters, $A$, $\pi$, and
$\theta$, to model the observed data. For convenience, I use the compact
notation:
[
\begin{align}
    Y &amp;amp;= \{y_{1}, \, y_{2}, \, \dots, y_{T}\},\\&lt;br /&gt;
    Z &amp;amp;= \{z_{1}, \, z_{2}, \, \dots, z_{T}\}, \quad{and}\\&lt;br /&gt;
    \lambda &amp;amp;= \{A, \, \pi, \, \theta\}.
\end{align}
]&lt;/p&gt;

&lt;p&gt;In practice, HMMs have to solve two problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Given the model parameters, what is the most likely sequence of latent states?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Given the observed data, what is the most likely parameter values?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To solve these problems, we need to know how to calculate the likelihood of the
observed data given the parameters.&lt;/p&gt;

&lt;h2 id=&#34;likelihood-computation&#34;&gt;Likelihood Computation&lt;/h2&gt;

&lt;p&gt;A naive approach to compute $p(Y \vert \lambda)$ is to simply integrate out $Z$:
[
\begin{align}
    p(Y \vert \lambda)
        &amp;amp;= \int p(Y, Z \vert \lambda) \, d_{Z}\\&lt;br /&gt;
        &amp;amp;= \int p(Y \vert Z, \, \lambda) \, p(Z \vert \lambda) \, d_{Z}\\&lt;br /&gt;
        &amp;amp;= \int \left( \prod_{t=1}^{T} f(y_{t} \vert \theta_{z_{t}}) \right)
        \left( p(z_{1}) \, \prod_{t=2}^{T} A_{z_{t-1}, z_{t}} \right) d_{Z}.
\end{align}
]
This integration over $Z$ may not be feasible, as the above equation involves on
the order of $2TK^{T}$ calculations. This is because at every $t = 1, 2,
\dots, T$, there are $K$ possible latent states, resulting in $K^{T}$
possible state sequence. In addition, for each state sequence, about $2T$
calculations are required.&lt;/p&gt;

&lt;p&gt;Fortunately, a more efficient procedure exists, which is called
&lt;em&gt;forward algorithm&lt;/em&gt;. We first define the forward variable,
[
    \alpha_{t}(i) = p(y_{1}, y_{2}, \dots, y_{t}, z_{t} = i \vert \lambda),
]
which is the probability of the partial observation sequence up to time $t$ and
latent state $i$ at time $t$.&lt;/p&gt;

&lt;p&gt;With this forward variable, the likelihood can be computed as
[
    p(Y \vert \lambda) = \sum_{i=1}^{K} \alpha_{T}(i),
]
where the forward variable is computed recursively:
[
    \alpha_{t}(i) =
        \begin{cases}
            \pi_{i} \, f(y_{t} \vert \theta_{i})
            &amp;amp; \textrm{for } t = 1
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \left(
                \sum_{j = 1}^{K} \alpha_{t-1}(j) \, A_{i, j}
            \right)
            f(y_{t} \vert \theta_{i})
            &amp;amp; \textrm{for } t = 2, 3, \dots, T.
        \end{cases}
]
As we need to calculate $\alpha_{t}(i)$ for each $i = 1, 2, \dots, K$,
the forward variable requires the order of $K^{2} T$ calculations, much less
than our naive approach.&lt;/p&gt;

&lt;h2 id=&#34;finding-the-sequence-of-latent-states-the-first-approach&#34;&gt;Finding the sequence of latent states: the first approach&lt;/h2&gt;

&lt;p&gt;Now we consider how to find the most likely sequence of latent states, given
the parameters. One simpler approach is to maximise the probability of latent
state at each time point:
[
    z_{t}^{*} = \arg \max_{i} p(z_{t} = i \vert Y, \, \lambda).
]&lt;/p&gt;

&lt;p&gt;To compute this probability, we define the backward variable,
[
    \beta_{t}(i) = p(y_{t+1}, y_{t+2}, \dots, y_{T} \vert z_{t} = i, \, \lambda),
]
which is the probability of the partial observation sequence from $t+1$, given
latent state $i$ at time $t$.  As with the forward variable, the backward
variable can be computed recursively
[
    \beta_{t}(i) =
        \begin{cases}
            \sum_{j = 1}^{K} A_{i, j} \, \beta_{t+1}(j) \, f(y_{t+1} \vert \theta_{j})
            &amp;amp; \textrm{for } t = {1, 2, \dots, T - 1}
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            1
            &amp;amp; \textrm{for } t = T.
        \end{cases}
]&lt;/p&gt;

&lt;p&gt;Then, the probability of latent state $i$ at time $t$ is given by
[
\begin{align}
    p(z_{t} = i \vert Y, \, \lambda) &amp;amp;=
        \frac{
            p(y_{1}, y_{2}, \dots, y_{t}, z_{t} = i \vert \lambda)
            \,
            p(y_{t+1}, y_{t+2}, \dots, y_{T} \vert z_{t} = i, \, \lambda)
        }{
            p(Y \vert \lambda)
        }
        \\&lt;br /&gt;
        &amp;amp;=
        \frac{
            \alpha_{t}(i)
            \,
            \beta_{t}(i)
        }{
            p(Y \vert \lambda).
        }
\tag{1}\label{eq:gamma}
\end{align}
]
The denominator does not depend on $z_{t}$, so we only need to consider
[
    p(z_{t} = i \vert Y, \, \lambda) \propto \alpha_{t}(i) \, \beta_{t}(i),
]
and the most likely state at time $t$ is
[
    \arg \max_{i} \alpha_{t}(i) \, \beta_{t}(i).
]&lt;/p&gt;

&lt;p&gt;This simpler approach considers the most likely state at each time point
independently, and thus, the resulting sequence can be very unlikely. For
example, suppose the approach finds that the latent state is $1$ at time $t$
and $2$ at time $t + 1$. But this estimation does not consider $A_{1, 2}$ and
is impossible when $A_{1,2} = 0$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Viterbi algorithm&lt;/em&gt;, we discuss next, considers multiple time points to find
the sequence of latent states that maximise $p(Z \vert y, \, \lambda)$.&lt;/p&gt;

&lt;h2 id=&#34;finding-the-sequence-of-latent-states-viterbi-algorithm&#34;&gt;Finding the sequence of latent states: Viterbi algorithm&lt;/h2&gt;

&lt;p&gt;With the Viterbi algorithm, we sequentially compute the probability of the most
probable state sequence which ends in latent state $i$ at time $t$,
[
    \delta_{t}(i) = \max_{z_{1}, z_{2}, \dots, z_{t - 1}}
        p(z_{1}, z_{2}, \dots, z_{t - 1}, z_{t} = i, y_{1}, y_{2}, \dots, y_{t} \vert \lambda).
]
This probability can be computed recursively
[
    \delta_{t}(i) =
        \begin{cases}
            \pi_{i} \, f(y_{t} \vert \theta_{i})
            &amp;amp; \textrm{for } t = 1
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \max_{j} \left( \delta_{t-1}(j) \, A_{j, i} \right)
                f(y_{t} \vert \theta_{j})
            &amp;amp; \textrm{for } t = {2, 3, \dots, T}.
        \end{cases}
]&lt;/p&gt;

&lt;p&gt;When computing $\delta$, we store the state used at each time $t$:
[
    \psi_{t}(i) =
        \begin{cases}
            0
            &amp;amp; \textrm{for } t = 1
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \arg \max_{j} \left( \delta_{t-1}(j) \, A_{j, i} \right)
            &amp;amp; \textrm{for } t = {2, 3, \dots, T}.
        \end{cases}
]
Then, the best state sequence can be found by backtracking,
[
    z_{t}^{*} =
        \begin{cases}
            \arg \max_{j} \delta_{t-1}(j) &amp;amp; \textrm{for } t = T
            \\&lt;br /&gt;
            \\&lt;br /&gt;
            \psi_{t+1}(z_{t+1}^{*}) &amp;amp; \textrm{for } t = T - 1, T - 2, \dots, 1.
        \end{cases}
]
This backtracking first identifies the latent state where the most probable
sequence ends up, $z_{T}^{*}$, and finds which latent state is most
likely preceded. Then it repeats this backtracking till the beginning of the
sequence.&lt;/p&gt;

&lt;h2 id=&#34;parameter-values&#34;&gt;Parameter values&lt;/h2&gt;

&lt;p&gt;There are several procedures to estimate parameters, but here, I review the
Baum-Welch algorithm, which uses the EM algorithm to find the maximum
likelihood estimate of the parameters.&lt;/p&gt;

&lt;p&gt;In order to describe the algorithm, I define the probability of being in the
latent state $i$ at time $t$ and latent state $j$ at time $t+1$:
[
    \xi_{t}(i, j) = p(z_{t} = i,\, z_{t+1} = j \vert Y, \lambda).
]
Using the forward and backward variables,
[
\begin{align}
    \xi_{t}(i, j)
    &amp;amp;= \frac{
            p(z_{t} = i,\, z_{t+1} = j, \, Y \vert \lambda)
        }{
            p(Y \vert \lambda)
        }
    \\&lt;br /&gt;
    &amp;amp;= \frac{
        p(y_{1}, y_{2}, \dots, y_{t}, z_{t} = i \vert \lambda)
        \,
        p(z_{t+1} = j \vert z_{t} = i)
        \,
        p(y_{t+1} \vert z_{t+1} = j, \lambda)
        \,
        p(y_{t+2}, y_{t+3}, \dots, y_{T} \vert z_{t+1} = j, \, \lambda)
        }{
            p(Y \vert \lambda)
        }
    \\&lt;br /&gt;
    &amp;amp;= \frac{
            \alpha_{t}(i) \, A_{j,i} \, f(y_{t+1} \vert \theta_{j}) \, \beta_{t+1}(j)
        }{
            p(Y \vert \lambda)
        }
    \\&lt;br /&gt;
    &amp;amp;= \frac{
            \alpha_{t}(i) \, A_{j,i} \, f(y_{t+1} \vert \theta_{j}) \, \beta_{t+1}(j)
        }{
            \sum_{i=1}^{K}
            \sum_{j=1}^{K}
            \alpha_{t}(i) \, A_{j,i} \, f(y_{t+1} \vert \theta_{j}) \, \beta_{t+1}(j).
        }
\end{align}
]
Then, the probability of being in latent state $i$ at time $t$ is defined as
[
    \gamma_{t}(i) = \sum_{j=1}^{K} \xi_{t}(i, j).
]
Note that this $\gamma$ is a redifinition of Equation \ref{eq:gamma}: that is,
[
    \gamma_{t}(i)
        = p(z_{t} = i \vert Y, \, \lambda)
        =
        \frac{
            \alpha_{t}(i)
            \,
            \beta_{t}(i)
        }{
            p(Y \vert \lambda)
        }
]&lt;/p&gt;

&lt;p&gt;If we sum $\gamma_{t}(i)$ from $t = 1$ to $t = T -1$, we get the expected
number of transitions from latent state $i$. If we sum $\xi_{t}(i,j)$ over
$t$, we get the expected number of transitions from latent state $i$ to latent
state $j$.&lt;/p&gt;

&lt;p&gt;Now we can update the parameters:
[
\begin{align}
    \pi_{i} &amp;amp;\gets \gamma_{1}(i)
    \\&lt;br /&gt;
    a_{ij} &amp;amp;\gets \frac{
        \sum_{t=1}^{T-1} \xi_{t}(i, j)
        }{
        \sum_{t=1}^{T-1} \gamma_{t}(i)
    }.
\end{align}
]&lt;/p&gt;

&lt;p&gt;Similarly, $\theta$ can be updated. If $f$ is the probability mass function for
multinoulli distribution,
[
    \theta_{i,v} = p(y = v \vert z = i)
    \gets \frac{
        \sum_{t=1}^{T-1} \mathbb{1}_{y_{t} = v} \gamma_{t}(i)
        }{
        \sum_{t=1}^{T-1} \gamma_{t}(i)
    }.
]
Here, $\mathbb{1}$ is the indicator function.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;HMMs are implemented in
&lt;a href=&#34;http://scikit-learn.sourceforge.net/stable/modules/hmm.html&#34;&gt;scikit-learn&lt;/a&gt;,
but this scikit-learn module has been moved to a separate module
&lt;a href=&#34;https://github.com/hmmlearn/hmmlearn&#34;&gt;hmmlearn&lt;/a&gt;.  There are probably more out
there.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;Rabiner, L. R. (1998). A tutorial on hidden Markov models and selected
applications in speech recognition. &lt;em&gt;Proceedings of The IEEE, 77,&lt;/em&gt; 257-286.
&lt;a href=&#34;http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial
on hmm and applications.pdf&#34;&gt;link to pdf&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hierarchical Bayes</title>
      <link>https://tkngch.github.io/post/2016-12-26_Hierarchical-Bayes/</link>
      <pubDate>Mon, 26 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2016-12-26_Hierarchical-Bayes/</guid>
      <description>&lt;p&gt;Sometimes, data contain multiple entries from observation units. For example,
researchers may be interested in effectiveness of medical treatments, and
tested several treatments in several different cities. Then, it may make sense
to consider that the effectiveness may be overall consistent across cities but
slightly vary between the cities, as population characteristic may vary between
the cities. For instance, one city may have more younger people than another.
In these cases, hierarchical Bayes may help. In this blog post, I review
hierarchical Bayes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;For the illustration purposes, let&amp;rsquo;s say we want to fit a linear regression:
[
    y \sim \mathcal{N}(X \beta, \epsilon).
]
In the above example of medical treatments, each row in $X$ corresponds to one
observation. Here, $X$ indexes the type of treatment received, and $y$ is a
measure of effectiveness.&lt;/p&gt;

&lt;p&gt;For convenience, I make the condition on $X$ implicit in this blog post.&lt;/p&gt;

&lt;h2 id=&#34;complete-pooling&#34;&gt;Complete pooling&lt;/h2&gt;

&lt;p&gt;Before diving into hierarchical Bayes, it is useful to discuss simpler
approaches. One approach is to ignore the possibility that the effectiveness of
treatments may vary between the cities, and assume that the effectiveness is
identical across the cities. This approach is called &lt;em&gt;complete pooling&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;First, we define the prior distribution for the parameter:
[
    \beta \sim \mathcal{N}(\mu, \sigma).
]
Here, $\mu$ and $\sigma$ are hyperparameters &amp;mdash; something you have to specify.
How to choose hyper-parameters is a topic for another blog post.&lt;/p&gt;

&lt;p&gt;After specifying the prior distribution, we can optimise $\beta$ according to
its posterior distribution:
[
    p(\beta \vert y) = \frac{p(\beta) \; p(y \vert \beta)}{p(y).}
]
The denominator, $p(y)$, is usually difficult to compute, but it does not
depend on $\beta$, so often we just consider the unnormalised posterior:
[
    p(\beta \vert y) \propto p(\beta) \; p(y \vert \beta).
]&lt;/p&gt;

&lt;h2 id=&#34;no-pooling&#34;&gt;No pooling&lt;/h2&gt;

&lt;p&gt;Another simpler approach is to acknowledge the possibility that the
effectiveness of treatments varies between the cities and allow the
effectiveness to vary as much as possible between the cities. Thus, this
approach fits the model independently for each city. This approach is called
&lt;em&gt;no pooling&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Here the model for the $i$th city is
[
    y^{(i)} \sim \mathcal{N}(X^{(i)} \beta^{(i)}, \epsilon^{(i)}).
]&lt;/p&gt;

&lt;p&gt;We specify the prior as before:
[
    \beta^{(i)} \sim \mathcal{N}(\mu, \sigma).
]
And then, optimise $\beta^{(i)}$ according to the posterior $p(\beta^{(i)}
\vert y)$.&lt;/p&gt;

&lt;h2 id=&#34;partial-pooling-hierarchical-bayes&#34;&gt;Partial pooling (hierarchical Bayes)&lt;/h2&gt;

&lt;p&gt;In some cases, it makes sense to allow the effectiveness to vary between the
cities but also to assume that the effectiveness is somewhat consistent across
the cities. This is where the third approach, &lt;em&gt;partial pooling&lt;/em&gt; comes in. The
partial pooling is often called hierarchical Bayes.&lt;/p&gt;

&lt;p&gt;As before, let&amp;rsquo;s say the model for the $i$th city is
[
    y^{(i)} \sim X^{(i)} \beta^{(i)} + \epsilon^{(i)},
]
and the prior is
[
    \beta^{(i)} \sim \mathcal{N}(\mu, \sigma).
]&lt;/p&gt;

&lt;p&gt;The trick is to treat $\mu$ and $\sigma$ here as parameters, as opposed to
hyperparameters. That is, we let the data to inform us of $\mu$ and $\sigma$.
For this, we need to specify the prior for $\mu$ and $\sigma$. My usual choice is
[
    \mu \sim Uniform(-inf, inf)
]
and
[
    \sigma \sim Uniform(0, inf).
]
These are non-informative distributions. These are also improper, as their
integral is not one. While the prior is improper, the posterior integrates to
one, so it doesn&amp;rsquo;t cause much practical problems.&lt;/p&gt;

&lt;p&gt;Then, we can work out the joint posterior distribution.
[
\begin{align}
    p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y)
        &amp;amp;= \frac{p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma) \; p(y \vert \beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma)}{p(y)} \\&lt;br /&gt;
        &amp;amp;= \frac{p(\mu, \sigma) \; p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)} \vert \mu, \sigma) \; p(y \vert \beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)})}{p(y).}
\end{align}
]
As before, we often only need to consider the unnormalised posterior:
[
    p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y) \propto
        p(\mu, \sigma) \; p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)} \vert \mu, \sigma) \; p(y \vert \beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}).
]&lt;/p&gt;

&lt;p&gt;The overall effectiveness of treatments can be inferred by integrating out
$\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}$ from the above equation:
[
    p(\mu, \sigma \vert y) =
        \int
            p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y)
        d_{\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}}.
]
Similarly, the effectiveness for each city can be inferred by integrating out
$\mu$ and $\sigma$:
[
    p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)} \vert y) =
        \int \int
            p(\beta^{(1)}, \beta^{(2)}, \dots \beta^{(n)}, \mu, \sigma \vert y)
        d_{\mu} d_{\sigma}.
]&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dirichlet process mixture model as a cognitive model of category learning</title>
      <link>https://tkngch.github.io/post/2016-12-17/</link>
      <pubDate>Sat, 17 Dec 2016 15:30:10 +0000</pubDate>
      
      <guid>https://tkngch.github.io/post/2016-12-17/</guid>
      <description>&lt;p&gt;In cognitive science/psychology, Dirichlet process mixture model (DPMM) is
considered as a rational model of category learning (Anderson, 1991; Sanborn,
Griffiths &amp;amp; Navarro, 2010). That is, the DPMM is used to approximate how human
learns to categorise objects.&lt;/p&gt;

&lt;p&gt;In this post, I review the DPMM, assuming that all the features are discrete.
The implementation in C++ can be found
&lt;a href=&#34;https://github.com/tkngch/dirichlet-process-mixture-model&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Suppose a learner has observed $n - 1$ objects $\{x_{1}, x_{2}, \dots, x_{n -
1}\}$ with corresponding category labels $\{y_{1}, y_{2}, \dots, y_{n - 1}\}$.
In the DPMM, each of these objects fits into a cluster. The cluster label for
the $i$th object is denoted as $z_{i}$.&lt;/p&gt;

&lt;h2 id=&#34;drawing-an-inference&#34;&gt;Drawing an inference&lt;/h2&gt;

&lt;p&gt;Then, the probability that the $n$th object fits into category $w$ is expressed
as follows:&lt;/p&gt;

&lt;p&gt;[
\begin{align}
    p(y_{n} = w \; \vert \; x_{n})
    &amp;amp;= \sum_{k \in \mathbb{Z}} p(z_{n} = k \; \vert \; x_{n}) \; p(y_{n} = w \; \vert \; z_{n} = k) \\&lt;br /&gt;
    &amp;amp;= \sum_{k \in \mathbb{Z}} \; \frac{p(z_{n} = k) \; p(x_{n} \;\vert\; z_{n} = k)}{p(x_{n})} \; p(y_{n} = w \; \vert \; z_{n} =
    k) \\&lt;br /&gt;
    &amp;amp;= \sum_{k \in \mathbb{Z}} \; \frac{p(z_{n} = k) \; p(x_{n} \;\vert\; z_{n} = k)}{\sum_{s \in
    \mathbb{Z}} \; p(z_{n} = s) \; p(x_{n} \;\vert\; z_{n} = s)} \; p(y_{n} = w \; \vert \; z_{n} = k).
\label{eqn:inference} \tag{1}
\end{align}
]&lt;/p&gt;

&lt;p&gt;Here, $\mathbb{Z}$ is a set of all the possible clusters to which the $n$th
object can be assigned.  The three terms in Equation $\ref{eqn:inference}$ are
described below in turn.&lt;/p&gt;

&lt;p&gt;First, the probability that the $n$th object fits into a cluster $k$ is given
by:
[
    p(z_{n} = k) =
        \begin{cases}
            \frac{c\,m_{k}}{(1 - c) + c\,(n - 1)} &amp;amp; \text{if $m_{k} &amp;gt; 0$}\\&lt;br /&gt;
            \\&lt;br /&gt;
            \frac{(1 - c)}{(1 - c) + c\,(n - 1)} &amp;amp; \text{if $m_{k} = 0$}
        \end{cases}
\label{eqn:prior}\tag{2}
]
where $c$ is a parameter called the coupling probability and $m_{k}$ is the
number of objects assigned into cluster $k$.&lt;/p&gt;

&lt;p&gt;In typical experiments in cognitive science/psychology, each feature is
independent.  Therefore,
[
    p(x_{n} \; \vert \; z_{n} = k) = \prod_{d \in D} p(x_{n,d} \; \vert \; z_{n} = k),
\label{eqn:feature1}\tag{3}
]
where $D$ is a set of features that describe $x$.  The above term is computed
with
[
    p(x_{n,d} = v \; \vert \; z = k) = \frac{B_{v,d} + \beta_{c}}{m_{k} + J_{d} \, \beta_{c}},
\label{eqn:feature2}\tag{4}
]
where $B_{v,d}$ is the number of objects in cluster $k$ with value of $v$ on
feature $d$, and $J_{d}$ is the number of values which an object can take on
feature $d$.&lt;/p&gt;

&lt;p&gt;Similarly, the probability that the $n$th object has category label $w$, given
a cluster, is given by:
[
    p(y_{n} = w \; \vert \; z = k) = \frac{B_{w} + \beta_{l}}{B_{.} + J \, \beta_{l}},
\label{eqn:label}\tag{5}
]
where $B_{w}$ is the number of observed objects with category label $w$ in
cluster $k$, $B_{.}$ is the number of objects in cluster $k$, and $J$ is the
number of possible category labels.&lt;/p&gt;

&lt;h2 id=&#34;learning&#34;&gt;Learning&lt;/h2&gt;

&lt;p&gt;The learning in the DPMM is equivalent to assigning an object into a cluster.
The cluster assignment of the $n$th object, for example, is in accordance with
the probability of each cluster. Specifically, an object is assigned to a
cluster with the probability:
[
    p(z_{n} = k \;\vert\; x_{n},\, y_{n}) \propto
    p(z_{n} = k)
    \;
    p(x_{n} \;\vert\; z_{n} = k)
    \;
    p(y_{n} \;\vert\; z_{n} = k)
\label{eqn:learning}\tag{6}
]
This is computed with Equations $\ref{eqn:prior}$, $\ref{eqn:feature1}$ and
$\ref{eqn:label}$.&lt;/p&gt;

&lt;p&gt;This learning is mathematically intractable and approximated with local MAP
(Anderson, 1991) and particle filter (Sanborn et al., 2010).&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;Anderson, J. R. (1991). The adaptive nature of human categorization.
&lt;em&gt;Psychological Review, 98,&lt;/em&gt; 409-29.&lt;/p&gt;

&lt;p&gt;Sanborn, A. N., Griffiths, T. L., &amp;amp; Navarro, D. J. (2010). Rational
approximations to rational models: alternative algorithms for category
learning. &lt;em&gt;Psychological Review, 117,&lt;/em&gt; 1144-1167.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>